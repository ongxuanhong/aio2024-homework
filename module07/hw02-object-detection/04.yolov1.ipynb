{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hongong/anaconda3/envs/aio2024-homework/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to downloaded dataset: /home/hongong/.cache/kagglehub/datasets/andrewmvd/dog-and-cat-detection/versions/1\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "# download dataset\n",
    "data_dir = kagglehub.dataset_download(\"andrewmvd/dog-and-cat-detection\")\n",
    "print(\"Path to downloaded dataset:\", data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import patches\n",
    "from collections import Counter\n",
    "import cv2\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "from termcolor import colored\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomVOCDataset(torchvision.datasets.VOCDetection):\n",
    "    def __init__(self, class_mapping, S=7, B=2, C=20, custom_transforms=None):\n",
    "        self.S = S  # Grid size S x S\n",
    "        self.B = B  # Number of bounding boxes\n",
    "        self.C = C  # Number of classes\n",
    "        self.class_mapping = class_mapping  # Mapping of class names to class indices\n",
    "        self.custom_transforms = custom_transforms\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # get an image and its target (annotations) from the VOC dataset\n",
    "        image, target = super(CustomVOCDataset, self).__getitem__(index)\n",
    "        img_width, img_height = image.size\n",
    "\n",
    "        # convert target annotations to YOLO format bounding boxes\n",
    "        boxes = self.convert_to_yolo_format(\n",
    "            target, img_width, img_height, self.class_mapping\n",
    "        )\n",
    "        just_boxes = boxes[:, 1:]\n",
    "        labels = boxes[:, 0]\n",
    "\n",
    "        # transform\n",
    "        if self.custom_transforms:\n",
    "            sample = {\"image\": np.array(image), \"bboxes\": just_boxes, \"labels\": labels}\n",
    "            sample = self.custom_transforms(**sample)\n",
    "            image = sample[\"image\"]\n",
    "            boxes = sample[\"bboxes\"]\n",
    "            labels = sample[\"labels\"]\n",
    "\n",
    "        # create an empty label matrix for YOLO ground truth\n",
    "        label_matrix = torch.zeros((self.S, self.S, self.C + 5 * self.B))\n",
    "        boxes = torch.tensor(boxes, dtype=torch.float32)\n",
    "        labels = torch.tensor(labels, dtype=torch.float32)\n",
    "        image = torch.as_tensor(image, dtype=torch.float32)\n",
    "\n",
    "        # iterate through each bounding box in YOLO format\n",
    "        for box, class_label in zip(boxes, labels):\n",
    "            x, y, width, height = box.tolist()\n",
    "            class_label = int(class_label)\n",
    "\n",
    "            i, j = int(self.S * y), int(self.S * x)\n",
    "            x_cell, y_cell = self.S * x - j, self.S * y - i\n",
    "\n",
    "            # calculate the width and height of the box relative to the grid cell\n",
    "            width_cell, height_cell = (\n",
    "                width * self.S,\n",
    "                height * self.S,\n",
    "            )\n",
    "\n",
    "            # if no object has been found in this specific cell (i, j) before\n",
    "            if label_matrix[i, j, 20] == 0:\n",
    "                # mark that an object exists in this cell\n",
    "                label_matrix[i, j, 20] = 1\n",
    "\n",
    "                # store the box coordinates as an offset from the cell boundaries\n",
    "                box_coordinates = torch.tensor(\n",
    "                    [x_cell, y_cell, width_cell, height_cell]\n",
    "                )\n",
    "\n",
    "                # set the box coordinates in the label matrix\n",
    "                label_matrix[i, j, 21:25] = box_coordinates\n",
    "\n",
    "                # set the one-hot encoding for the class label\n",
    "                label_matrix[i, j, class_label] = 1\n",
    "\n",
    "        return image, label_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_yolo_format(target, img_width, img_height, class_mapping):\n",
    "    \"\"\"\n",
    "    Convert annotation data from VOC format to YOLO format\n",
    "\n",
    "    Parameters:\n",
    "        target (dict): annotation data from VOCDetection dataset.\n",
    "        img_width (int): width of the image.\n",
    "        img_height (int): height of the image.\n",
    "        class_mapping (dict): mapping of class names to class to interger IDS.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Tensor of shape [N, 5],  for N bounding boxes.\n",
    "                    each with [class_id, x_center, y_center, width, height]\n",
    "    \"\"\"\n",
    "\n",
    "    # Extract the list of a n n o t a t i o n s from the target dic ti ona ry .\n",
    "    annotations = target[\"annotation\"][\"object\"]\n",
    "\n",
    "    # get the real width and height of the image from the annotation\n",
    "    real_width = int(target[\"annotation\"][\"size\"][\"width\"])\n",
    "    real_height = int(target[\"annotation\"][\"size\"][\"height\"])\n",
    "\n",
    "    # ensure that annotations is a list, even if there's only one object\n",
    "    if not isinstance(annotations, list):\n",
    "        annotations = [annotations]\n",
    "\n",
    "    # initialize an empty list to store the converted bounding boxes\n",
    "    boxes = []\n",
    "\n",
    "    # loop through each annotation and convert it to YOLO format\n",
    "    for anno in annotations:\n",
    "        xmin = int(anno[\"bndbox\"][\"xmin\"]) / real_width\n",
    "        xmax = int(anno[\"bndbox\"][\"xmax\"]) / real_width\n",
    "        ymin = int(anno[\"bndbox\"][\"ymin\"]) / real_height\n",
    "        ymax = int(anno[\"bndbox\"][\"ymax\"]) / real_height\n",
    "\n",
    "        # Calculate the center coordinates , width , and height of the bounding box\n",
    "        x_center = (xmin + xmax) / 2\n",
    "        y_center = (ymin + ymax) / 2\n",
    "        width = xmax - xmin\n",
    "        height = ymax - ymin\n",
    "\n",
    "        # retrieve the class name from the annotation and map it to an interger ID\n",
    "        class_name = anno[\"name\"]\n",
    "        class_id = class_mapping[class_name] if class_name in class_mapping else 0\n",
    "\n",
    "        # append the YOLO formatted bbox to the list.\n",
    "        boxes.append([class_id, x_center, y_center, width, height])\n",
    "\n",
    "    # convert the list of  boxes to a torch tensor\n",
    "    return np.array(boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aio2024-homework",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
