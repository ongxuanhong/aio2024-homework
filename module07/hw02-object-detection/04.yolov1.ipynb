{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hongong/anaconda3/envs/aio2024-homework/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to downloaded dataset: /home/hongong/.cache/kagglehub/datasets/andrewmvd/dog-and-cat-detection/versions/1\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "# download dataset\n",
    "data_dir = kagglehub.dataset_download(\"andrewmvd/dog-and-cat-detection\")\n",
    "print(\"Path to downloaded dataset:\", data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import patches\n",
    "from collections import Counter\n",
    "import cv2\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "from termcolor import colored\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomVOCDataset(torchvision.datasets.VOCDetection):\n",
    "    def __init__(self, class_mapping, S=7, B=2, C=20, custom_transforms=None):\n",
    "        self.S = S  # Grid size S x S\n",
    "        self.B = B  # Number of bounding boxes\n",
    "        self.C = C  # Number of classes\n",
    "        self.class_mapping = class_mapping  # Mapping of class names to class indices\n",
    "        self.custom_transforms = custom_transforms\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # get an image and its target (annotations) from the VOC dataset\n",
    "        image, target = super(CustomVOCDataset, self).__getitem__(index)\n",
    "        img_width, img_height = image.size\n",
    "\n",
    "        # convert target annotations to YOLO format bounding boxes\n",
    "        boxes = self.convert_to_yolo_format(\n",
    "            target, img_width, img_height, self.class_mapping\n",
    "        )\n",
    "        just_boxes = boxes[:, 1:]\n",
    "        labels = boxes[:, 0]\n",
    "\n",
    "        # transform\n",
    "        if self.custom_transforms:\n",
    "            sample = {\"image\": np.array(image), \"bboxes\": just_boxes, \"labels\": labels}\n",
    "            sample = self.custom_transforms(**sample)\n",
    "            image = sample[\"image\"]\n",
    "            boxes = sample[\"bboxes\"]\n",
    "            labels = sample[\"labels\"]\n",
    "\n",
    "        # create an empty label matrix for YOLO ground truth\n",
    "        label_matrix = torch.zeros((self.S, self.S, self.C + 5 * self.B))\n",
    "        boxes = torch.tensor(boxes, dtype=torch.float32)\n",
    "        labels = torch.tensor(labels, dtype=torch.float32)\n",
    "        image = torch.as_tensor(image, dtype=torch.float32)\n",
    "\n",
    "        # iterate through each bounding box in YOLO format\n",
    "        for box, class_label in zip(boxes, labels):\n",
    "            x, y, width, height = box.tolist()\n",
    "            class_label = int(class_label)\n",
    "\n",
    "            i, j = int(self.S * y), int(self.S * x)\n",
    "            x_cell, y_cell = self.S * x - j, self.S * y - i\n",
    "\n",
    "            # calculate the width and height of the box relative to the grid cell\n",
    "            width_cell, height_cell = (\n",
    "                width * self.S,\n",
    "                height * self.S,\n",
    "            )\n",
    "\n",
    "            # if no object has been found in this specific cell (i, j) before\n",
    "            if label_matrix[i, j, 20] == 0:\n",
    "                # mark that an object exists in this cell\n",
    "                label_matrix[i, j, 20] = 1\n",
    "\n",
    "                # store the box coordinates as an offset from the cell boundaries\n",
    "                box_coordinates = torch.tensor(\n",
    "                    [x_cell, y_cell, width_cell, height_cell]\n",
    "                )\n",
    "\n",
    "                # set the box coordinates in the label matrix\n",
    "                label_matrix[i, j, 21:25] = box_coordinates\n",
    "\n",
    "                # set the one-hot encoding for the class label\n",
    "                label_matrix[i, j, class_label] = 1\n",
    "\n",
    "        return image, label_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_yolo_format(target, img_width, img_height, class_mapping):\n",
    "    \"\"\"\n",
    "    Convert annotation data from VOC format to YOLO format\n",
    "\n",
    "    Parameters:\n",
    "        target (dict): annotation data from VOCDetection dataset.\n",
    "        img_width (int): width of the image.\n",
    "        img_height (int): height of the image.\n",
    "        class_mapping (dict): mapping of class names to class to interger IDS.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Tensor of shape [N, 5],  for N bounding boxes.\n",
    "                    each with [class_id, x_center, y_center, width, height]\n",
    "    \"\"\"\n",
    "\n",
    "    # Extract the list of a n n o t a t i o n s from the target dic ti ona ry .\n",
    "    annotations = target[\"annotation\"][\"object\"]\n",
    "\n",
    "    # get the real width and height of the image from the annotation\n",
    "    real_width = int(target[\"annotation\"][\"size\"][\"width\"])\n",
    "    real_height = int(target[\"annotation\"][\"size\"][\"height\"])\n",
    "\n",
    "    # ensure that annotations is a list, even if there's only one object\n",
    "    if not isinstance(annotations, list):\n",
    "        annotations = [annotations]\n",
    "\n",
    "    # initialize an empty list to store the converted bounding boxes\n",
    "    boxes = []\n",
    "\n",
    "    # loop through each annotation and convert it to YOLO format\n",
    "    for anno in annotations:\n",
    "        xmin = int(anno[\"bndbox\"][\"xmin\"]) / real_width\n",
    "        xmax = int(anno[\"bndbox\"][\"xmax\"]) / real_width\n",
    "        ymin = int(anno[\"bndbox\"][\"ymin\"]) / real_height\n",
    "        ymax = int(anno[\"bndbox\"][\"ymax\"]) / real_height\n",
    "\n",
    "        # Calculate the center coordinates , width , and height of the bounding box\n",
    "        x_center = (xmin + xmax) / 2\n",
    "        y_center = (ymin + ymax) / 2\n",
    "        width = xmax - xmin\n",
    "        height = ymax - ymin\n",
    "\n",
    "        # retrieve the class name from the annotation and map it to an interger ID\n",
    "        class_name = anno[\"name\"]\n",
    "        class_id = class_mapping[class_name] if class_name in class_mapping else 0\n",
    "\n",
    "        # append the YOLO formatted bbox to the list.\n",
    "        boxes.append([class_id, x_center, y_center, width, height])\n",
    "\n",
    "    # convert the list of  boxes to a torch tensor\n",
    "    return np.array(boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intersection_over_union(boxes_preds, boxes_labels, box_format=\"midpoint\"):\n",
    "    \"\"\"\n",
    "    Calculate intersection over union (Jaccard overlap) between prediction and target boxes\n",
    "\n",
    "    Parameters:\n",
    "        boxes_preds (torch.Tensor): Predictions of Bounding boxes (BATCH_SIZE, 4)\n",
    "        boxes_labels (torch.Tensor): Correct labels of Bounding boxes (BATCH_SIZE, 4)\n",
    "        box_format (str): midpoint/corners, if boxes (x,y,w,h) or (x1,y1,x2,y2)\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Intersection over union for all examples\n",
    "    \"\"\"\n",
    "\n",
    "    # check if the box format is \"midpoint\"\n",
    "    if box_format == \"midpoint\":\n",
    "        # calculate corrdinates of top-left (x1, y1) and bottom-right (x2, y2) points for predicted boxes\n",
    "        box1_x1 = boxes_preds[..., 0:1] - boxes_preds[..., 2:3] / 2\n",
    "        box1_y1 = boxes_preds[..., 1:2] - boxes_preds[..., 3:4] / 2\n",
    "        box1_x2 = boxes_preds[..., 0:1] + boxes_preds[..., 2:3] / 2\n",
    "        box1_y2 = boxes_preds[..., 1:2] + boxes_preds[..., 3:4] / 2\n",
    "\n",
    "        # calculate corrdinates of top-left (x1, y1) and bottom-right (x2, y2) points for ground truth boxes\n",
    "        box2_x1 = boxes_labels[..., 0:1] - boxes_labels[..., 2:3] / 2\n",
    "        box2_y1 = boxes_labels[..., 1:2] - boxes_labels[..., 3:4] / 2\n",
    "        box2_x2 = boxes_labels[..., 0:1] + boxes_labels[..., 2:3] / 2\n",
    "        box2_y2 = boxes_labels[..., 1:2] + boxes_labels[..., 3:4] / 2\n",
    "\n",
    "    # check if the box format is \"corners\"\n",
    "    if box_format == \"corners\":\n",
    "        # calculate corrdinates for predicted boxes\n",
    "        box1_x1 = boxes_preds[..., 0:1]\n",
    "        box1_y1 = boxes_preds[..., 1:2]\n",
    "        box1_x2 = boxes_preds[..., 2:3]\n",
    "        box1_y2 = boxes_preds[..., 3:4]\n",
    "\n",
    "        # calculate corrdinates for ground truth boxes\n",
    "        box2_x1 = boxes_labels[..., 0:1]\n",
    "        box2_y1 = boxes_labels[..., 1:2]\n",
    "        box2_x2 = boxes_labels[..., 2:3]\n",
    "        box2_y2 = boxes_labels[..., 3:4]\n",
    "\n",
    "    # calculate the corrdinates of the intersection rectangle\n",
    "    x1 = torch.max(box1_x1, box2_x1)\n",
    "    y1 = torch.max(box1_y1, box2_y1)\n",
    "    x2 = torch.min(box1_x2, box2_x2)\n",
    "    y2 = torch.min(box1_y2, box2_y2)\n",
    "\n",
    "    # calculate the area of intersection rectangle\n",
    "    intersection = (x2 - x1).clamp(0) * (y2 - y1).clamp(0)\n",
    "\n",
    "    # calculate the area of both prediction and ground truth boxes\n",
    "    box1_area = abs((box1_x2 - box1_x1) * (box1_y2 - box1_y1))\n",
    "    box2_area = abs((box2_x2 - box2_x1) * (box2_y2 - box2_y1))\n",
    "\n",
    "    # calculate the area of union of both boxes, add small epsilon to avoid division by zero\n",
    "    return intersection / (box1_area + box2_area - intersection + 1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def non_max_suppression(boxes, iou_threshold, threshold, box_format=\"corners\"):\n",
    "    \"\"\"\n",
    "    Perform Non-maxium Suppression on a list of bbox.\n",
    "\n",
    "    Parameters:\n",
    "        boxes (list): list of lists containing all bboxes with each bboxes [class_pred, prob_score, x1, y1, x2, y2]\n",
    "        specified as [class_pred, prob_score, x1, y1, x2, y2]\n",
    "        iou_threshold (float): threshold where predicted bboxes is correct\n",
    "        threshold (float): threshold to remove predicted bboxes (independent of IoU)\n",
    "        box_format (str): \"midpoint\" or \"corners\" used to specify bboxes\n",
    "\n",
    "    Returns:\n",
    "        list: bboxes after performing NMS given a specific IoU threshold\n",
    "    \"\"\"\n",
    "\n",
    "    # check the data type of the input parameter\n",
    "    assert type(bboxes) == list\n",
    "\n",
    "    # filter predicted bbox based on probability threshold\n",
    "    bboxes = [box for box in bboxes if box[1] > threshold]\n",
    "\n",
    "    # sort bbox by their probability in descending order\n",
    "    bboxes = sorted(bboxes, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # list to store bbox after NMS\n",
    "    bboxes_after_nms = []\n",
    "\n",
    "    # continue looping until the list of bbox is empty\n",
    "    while bboxes:\n",
    "        # pop the bbox with the highest probability\n",
    "        chosen_box = bboxes.pop(0)\n",
    "\n",
    "        # remove bbox with IoU higher than the threshold\n",
    "        bboxes = [\n",
    "            box\n",
    "            for box in bboxes\n",
    "            if box[0] != chosen_box[0]\n",
    "            or intersection_over_union(\n",
    "                torch.tensor(chosen_box[2:]),\n",
    "                torch.tensor(box[2:]),\n",
    "                box_format=box_format,\n",
    "            )\n",
    "            < iou_threshold\n",
    "        ]\n",
    "\n",
    "        # add the chosen bbox to the list of bbox after NMS\n",
    "        bboxes_after_nms.append(chosen_box)\n",
    "\n",
    "    return bboxes_after_nms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_average_precision(\n",
    "    pred_boxes, true_boxes, iou_threshold=0.5, box_format=\"midpoint\", num_classes=20\n",
    "):\n",
    "    \"\"\"\n",
    "    Calculate mean average precision for object detection (mAP)\n",
    "\n",
    "    Parameters:\n",
    "        pred_boxes (list): list of lists containing all bboxes with each bboxes [class_pred, prob_score, x1, y1, x2, y2]\n",
    "        true_boxes (list): list of lists containing all bboxes with each bboxes [class_pred, x, y, width, height]\n",
    "        iou_threshold (float): threshold where predicted bboxes is correct\n",
    "        box_format (str): \"midpoint\" or \"corners\" used to specify bboxes\n",
    "        num_classes (int): number of classes\n",
    "\n",
    "    Returns:\n",
    "        float: mAP value across all classes given a specific IoU threshold\n",
    "    \"\"\"\n",
    "\n",
    "    # list to store mAP for each class\n",
    "    average_precisions = []\n",
    "\n",
    "    # small epsilon value to avoid division by zero\n",
    "    epsilon = 1e-6\n",
    "\n",
    "    for c in range(num_classes):\n",
    "        detections = []\n",
    "        ground_truths = []\n",
    "\n",
    "        # iterate through all predictions and targets, and only add those belonging to the current class 'c'\n",
    "        for detection in pred_boxes:\n",
    "            if detection[0] == c:\n",
    "                detections.append(detection)\n",
    "\n",
    "        for true_box in true_boxes:\n",
    "            if true_box[0] == c:\n",
    "                ground_truths.append(true_box)\n",
    "\n",
    "        # find the number of boxes for each training example.\n",
    "        # the Counter here counts the number of target boxes we have\n",
    "        # for each training example, so if image 0 has 3, and image 1 has 5,,\n",
    "        # we'll have a dictionary with {0: 3, 1: 5}\n",
    "        amount_bboxes = Counter([gt[0] for gt in ground_truths])\n",
    "\n",
    "        # we then loop through each key, val in this dictionary and convert it to the following (for the same example):\n",
    "        # {0: torch.tensor([0, 0, 0]), 1: torch.tensor([0, 0, 0, 0, 0])}\n",
    "        for key, val in amount_bboxes.items():\n",
    "            amount_bboxes[key] = torch.zeros(val)\n",
    "\n",
    "        # sort the detections by their probability value, index 2 is the probability\n",
    "        detections.sort(key=lambda x: x[2], reverse=True)\n",
    "        TP = torch.zeros((len(detections)))\n",
    "        FP = torch.zeros((len(detections)))\n",
    "        total_true_bboxes = len(ground_truths)\n",
    "\n",
    "        # if there are no ground truth boxes for this class, it can be safely skipped\n",
    "        if total_true_bboxes == 0:\n",
    "            continue\n",
    "\n",
    "        for detection_idx, detection in enumerate(detections):\n",
    "            # only consider ground truth boxes with the same training index as the prediction\n",
    "            ground_truth_img = [\n",
    "                bbox for bbox in ground_truths if bbox[0] == detection[0]\n",
    "            ]\n",
    "\n",
    "            num_gts = len(ground_truth_img)\n",
    "            best_iou = 0\n",
    "\n",
    "            for idx, gt in enumerate(ground_truth_img):\n",
    "                iou = intersection_over_union(\n",
    "                    torch.tensor(detection[3:]),\n",
    "                    torch.tensor(gt[3:]),\n",
    "                    box_format=box_format,\n",
    "                )\n",
    "\n",
    "                if iou > best_iou:\n",
    "                    best_iou = iou\n",
    "                    best_gt_idx = idx\n",
    "\n",
    "            if best_iou > iou_threshold:\n",
    "                # only detect ground truth detection once\n",
    "                if amount_bboxes[detection[0]][best_gt_idx] == 0:\n",
    "                    # true positive and add this bounding box to seen\n",
    "                    TP[detection_idx] = 1\n",
    "                    amount_bboxes[detection[0]][best_gt_idx] = 1\n",
    "                else:\n",
    "                    FP[detection_idx] = 1\n",
    "            # if the IoU is lower then the detection is a false positive\n",
    "            else:\n",
    "                FP[detection_idx] = 1\n",
    "\n",
    "        # compute cumulative precision and recall\n",
    "        TP_cumsum = torch.cumsum(TP, dim=0)\n",
    "        FP_cumsum = torch.cumsum(FP, dim=0)\n",
    "        recalls = TP_cumsum / (total_true_bboxes + epsilon)\n",
    "        precisions = torch.divide(TP_cumsum, (TP_cumsum + FP_cumsum + epsilon))\n",
    "        precisions = torch.cat((torch.tensor([1]), precisions))\n",
    "        recalls = torch.cat((torch.tensor([0]), recalls))\n",
    "\n",
    "        # use torch.trapz for numerical integration\n",
    "        average_precisions.append(torch.trapz(precisions, recalls))\n",
    "\n",
    "    return sum(average_precisions) / len(average_precisions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Information about the architectural configuration:\n",
    "A tuple is structured as (kernel_size, number of filters, stride, padding).\n",
    "\"M\" simply represents max-pooling with a 2x2 pool size and 2x2 kernel.\n",
    "The lis is structured according to the data blocks, and ends with an integer representing the number of repetitions.\n",
    "\"\"\"\n",
    "\n",
    "# describing convolutional and max-pooling layers, as we as the number of repetitions of convolutional blocks.\n",
    "\n",
    "architecture_config = [\n",
    "    (7, 64, 2, 3),  # convolutional block 1\n",
    "    \"M\",  # max-pooling layer 1\n",
    "    (3, 192, 1, 1),  # convolutional block 2\n",
    "    \"M\",  # max-pooling layer 2\n",
    "    (1, 128, 1, 0),  # convolutional block 3\n",
    "    (3, 256, 1, 1),  # convolutional block 4\n",
    "    (1, 256, 1, 0),  # convolutional block 5\n",
    "    (3, 512, 1, 1),  # convolutional block 6\n",
    "    \"M\",  # max-pooling layer 3\n",
    "    [(1, 256, 1, 0), (3, 512, 1, 1), 4],  # convolutional block 7 (repeated 4 times)\n",
    "    (1, 512, 1, 0),  # convolutional block 8\n",
    "    (3, 1024, 1, 1),  # convolutional block 9\n",
    "    \"M\",  # max-pooling layer 4\n",
    "    [(1, 512, 1, 0), (3, 1024, 1, 1), 2],  # convolutional block 10 (repeated 2 times)\n",
    "    (3, 1024, 1, 1),  # convolutional block 11\n",
    "    (3, 1024, 2, 1),  # convolutional block 12\n",
    "    (3, 1024, 1, 1),  # convolutional block 13\n",
    "    (3, 1024, 1, 1),  # convolutional block 14\n",
    "]\n",
    "\n",
    "\n",
    "# a convolutional block is defined with Conv2d, BatchNorm2d, and LeakyReLU layers.\n",
    "class CNNBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, **kwargs):\n",
    "        super(CNNBlock, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, bias=False, **kwargs)\n",
    "        self.batchnorm = nn.BatchNorm2d(out_channels)\n",
    "        self.leakyrelu = nn.LeakyReLU(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.leakyrelu(self.batchnorm(self.conv(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The YOLOv1 model is defined with convolutional layers and fully connected layers.\n",
    "class Yolov1(nn.Module):\n",
    "    def __init__(self, in_channels=3, **kwargs):\n",
    "        super(Yolov1, self).__init__()\n",
    "        self.architecture = architecture_config\n",
    "        self.in_channels = in_channels\n",
    "        self.darknet = self._create_conv_layers(self.architecture)\n",
    "        self.fcs = self._create_fcs(**kwargs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.darknet(x)\n",
    "        return self.fcs(torch.flatten(x, start_dim=1))\n",
    "\n",
    "    # function to create convolutional layers based on the predefined architecture\n",
    "    def _create_conv_layers(self, architecture):\n",
    "        layers = []\n",
    "        in_channels = self.in_channels\n",
    "\n",
    "        for x in architecture:\n",
    "            if type(x) == tuple:\n",
    "                layers += [\n",
    "                    CNNBlock(\n",
    "                        in_channels,\n",
    "                        x[1],\n",
    "                        kernel_size=x[0],\n",
    "                        stride=x[2],\n",
    "                        padding=x[3],\n",
    "                    )\n",
    "                ]\n",
    "                in_channels = x[1]\n",
    "\n",
    "            elif type(x) == str:\n",
    "                layers += [nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2))]\n",
    "\n",
    "            elif type(x) == list:\n",
    "                conv1 = x[0]\n",
    "                conv2 = x[1]\n",
    "                num_repeats = x[2]\n",
    "\n",
    "                for _ in range(num_repeats):\n",
    "                    layers += [\n",
    "                        CNNBlock(\n",
    "                            in_channels,\n",
    "                            conv1[1],\n",
    "                            kernel_size=conv1[0],\n",
    "                            stride=conv1[2],\n",
    "                            padding=conv1[3],\n",
    "                        )\n",
    "                    ]\n",
    "                    layers += [\n",
    "                        CNNBlock(\n",
    "                            conv1[1],\n",
    "                            conv2[1],\n",
    "                            kernel_size=conv2[0],\n",
    "                            stride=conv2[2],\n",
    "                            padding=conv2[3],\n",
    "                        )\n",
    "                    ]\n",
    "                    in_channels = conv2[1]\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    # function to create fully connected layers based on the input parameters\n",
    "    # such as grid size, number of boxes, and number of classes\n",
    "    def _create_fcs(self, split_size, num_boxes, num_classes):\n",
    "        S, B, C = split_size, num_boxes, num_classes\n",
    "        return nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(1024 * S * S, 4096),\n",
    "            nn.Dropout(0.0),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Linear(4096, S * S * (C + B * 5)),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YoloLoss(nn.Module):\n",
    "    def __init__(self, S=7, B=2, C=20):\n",
    "        super(YoloLoss, self).__init__()\n",
    "        self.mse = nn.MSELoss(reduction=\"sum\")\n",
    "\n",
    "        \"\"\"\n",
    "        S is the grid size of the image (7)\n",
    "        B is the number of bounding boxes (2)\n",
    "        C is the number of classes (in VOC dataset, it's 20)\n",
    "        \"\"\"\n",
    "        self.S = S\n",
    "        self.B = B\n",
    "        self.C = C\n",
    "\n",
    "        # these are YOLO-specific constants, representing the weights\n",
    "        # for no object loss (lambda_noobj), and box coordinate loss (lambda_coord)\n",
    "        self.lambda_noobj = 0.5\n",
    "        self.lambda_coord = 5\n",
    "\n",
    "    def forward(self, predictions, target):\n",
    "        # reshape the predictions to the shape (BATCH_SIZE, S*S(C+B*5))\n",
    "        predictions = predictions.reshape(-1, self.S, self.S, self.C + self.B * 5)\n",
    "\n",
    "        # calculate IoU for the two bounding boxes\n",
    "        iou_b1 = intersection_over_union(predictions[..., 21:25], target[..., 21:25])\n",
    "        iou_b2 = intersection_over_union(predictions[..., 26:30], target[..., 21:25])\n",
    "        ious = torch.cat([iou_b1.unsqueeze(0), iou_b2.unsqueeze(0)], dim=0)\n",
    "\n",
    "        # get the box with the highest IoU among the two prediction\n",
    "        # note that bestbox will be 0 or 1, indicating which box is better\n",
    "        iou_maxes, bestbox = torch.max(ious, dim=0)\n",
    "\n",
    "        # this represents Iobj_i in the paper\n",
    "        exists_box = target[..., 20].unsqueeze(3)\n",
    "\n",
    "        # =================== #\n",
    "        # FOR BOX COORDINATES #\n",
    "        # =================== #\n",
    "\n",
    "        # set the boxes with no objects to zero. Choose one of the two predictions based on the best IoU\n",
    "        box_predictions = exists_box * (\n",
    "            (\n",
    "                bestbox * predictions[..., 26:30]\n",
    "                + (1 - bestbox) * predictions[..., 21:25]\n",
    "            )\n",
    "        )\n",
    "        box_targets = exists_box * target[..., 21:25]\n",
    "\n",
    "        # take the square root of the width and height to ensure positive values.\n",
    "        box_predictions[..., 2:4] = torch.sign(box_predictions[..., 2:4]) * torch.sqrt(\n",
    "            torch.abs(box_predictions[..., 2:4] + 1e-6)\n",
    "        )\n",
    "        box_targets[..., 2:4] = torch.sqrt(box_targets[..., 2:4])\n",
    "\n",
    "        box_loss = self.mse(\n",
    "            torch.flatten(box_predictions, end_dim=-2),\n",
    "            torch.flatten(box_targets, end_dim=-2),\n",
    "        )\n",
    "\n",
    "        # ================== #\n",
    "        # FOR OBJECT LOSS    #\n",
    "        # ================== #\n",
    "\n",
    "        # pred_box represents the confidence score for the box with the highest IoU\n",
    "        pred_box = (\n",
    "            bestbox * predictions[..., 25:26] + (1 - bestbox) * predictions[..., 20:21]\n",
    "        )\n",
    "        object_loss = self.mse(\n",
    "            torch.flatten(exists_box * pred_box),\n",
    "            torch.flatten(exists_box * target[..., 20:21]),\n",
    "        )\n",
    "\n",
    "        # ==================== #\n",
    "        # FOR NO OBJECT LOSS   #\n",
    "        # ==================== #\n",
    "        no_object_loss = self.mse(\n",
    "            torch.flatten((1 - exists_box) * predictions[..., 20:21], start_dim=1),\n",
    "            torch.flatten((1 - exists_box) * target[..., 20:21], start_dim=1),\n",
    "        )\n",
    "        no_object_loss += self.mse(\n",
    "            torch.flatten((1 - exists_box) * predictions[..., 25:26], start_dim=1),\n",
    "            torch.flatten((1 - exists_box) * target[..., 20:21], start_dim=1),\n",
    "        )\n",
    "\n",
    "        # ================== #\n",
    "        # FOR CLASS LOSS     #\n",
    "        # ================== #\n",
    "\n",
    "        class_loss = self.mse(\n",
    "            torch.flatten(\n",
    "                exists_box * predictions[..., :20],\n",
    "                end_dim=-2,\n",
    "            ),\n",
    "            torch.flatten(\n",
    "                exists_box * target[..., :20],\n",
    "                end_dim=-2,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        # calculate the final loss by combining the above losses\n",
    "        loss = (\n",
    "            self.lambda_coord * box_loss\n",
    "            + object_loss\n",
    "            + self.lambda_noobj * no_object_loss\n",
    "            + class_loss\n",
    "        )\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the random seed for reproducibility\n",
    "seed = 123\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# Hyperparameters and configurations\n",
    "# Learning rate for the optimizer\n",
    "LEARNING_RATE = 2e-5\n",
    "\n",
    "# Specify whether to use \"cuda\" (GPU) or \"cpu\" for training\n",
    "DEVICE = \"cuda\"\n",
    "\n",
    "# Originally 64 in the research paper, but using a smaller batch size due to GPU limitations\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "# Number of training epochs\n",
    "EPOCHS = 300\n",
    "\n",
    "# Number of worker processes for data loading\n",
    "NUM_WORKERS = 2\n",
    "\n",
    "# If True, DataLoader will pin memory to transfer data to the GPU faster\n",
    "PIN_MEMORY = True\n",
    "\n",
    "# If False, the training process will not load a pre-trained model\n",
    "LOAD_MODEL = False\n",
    "\n",
    "# Specify the file name for the pre-trained model if LOAD_MODEL is True\n",
    "LOAD_MODEL_FILE = \"yolov1.pth.tar\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "WIDTH = 448\n",
    "HEIGHT = 448\n",
    "\n",
    "\n",
    "def get_train_transforms():\n",
    "    return A.Compose(\n",
    "        [\n",
    "            A.OneOf(\n",
    "                [\n",
    "                    A.HueSaturationValue(\n",
    "                        hue_shift_limit=0.2,\n",
    "                        sat_shift_limit=0.2,\n",
    "                        val_shift_limit=0.2,\n",
    "                        p=0.9,\n",
    "                    ),\n",
    "                    A.RandomBrightnessContrast(\n",
    "                        brightness_limit=0.2, contrast_limit=0.2, p=0.9\n",
    "                    ),\n",
    "                ],\n",
    "                p=0.9,\n",
    "            ),\n",
    "            A.ToGray(p=0.01),\n",
    "            A.HorizontalFlip(p=0.2),\n",
    "            A.VerticalFlip(p=0.2),\n",
    "            A.Resize(height=WIDTH, width=WIDTH, p=1),\n",
    "            # A.Cutout(num_holes=8, max_h_size=64, max_w_size=64, fill_value=0, p=0.5),\n",
    "            ToTensorV2(p=1.0),\n",
    "        ],\n",
    "        p=1.0,\n",
    "        bbox_params=A.BboxParams(\n",
    "            format=\"yolo\", min_area=0, min_visibility=0, label_fields=[\"labels\"]\n",
    "        ),\n",
    "    )\n",
    "\n",
    "\n",
    "def get_valid_transforms():\n",
    "    return A.Compose(\n",
    "        [\n",
    "            A.Resize(height=WIDTH, width=WIDTH, p=1.0),\n",
    "            ToTensorV2(p=1.0),\n",
    "        ],\n",
    "        p=1.0,\n",
    "        bbox_params=A.BboxParams(\n",
    "            format=\"yolo\", min_area=0, min_visibility=0, label_fields=[\"labels\"]\n",
    "        ),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_mapping = {\n",
    "    \"aeroplane\": 0,\n",
    "    \"bicycle\": 1,\n",
    "    \"bird\": 2,\n",
    "    \"boat\": 3,\n",
    "    \"bottle\": 4,\n",
    "    \"bus\": 5,\n",
    "    \"car\": 6,\n",
    "    \"cat\": 7,\n",
    "    \"chair\": 8,\n",
    "    \"cow\": 9,\n",
    "    \"diningtable\": 10,\n",
    "    \"dog\": 11,\n",
    "    \"horse\": 12,\n",
    "    \"motorbike\": 13,\n",
    "    \"person\": 14,\n",
    "    \"pottedplant\": 15,\n",
    "    \"sheep\": 16,\n",
    "    \"sofa\": 17,\n",
    "    \"train\": 18,\n",
    "    \"tvmonitor\": 19,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(train_loader, model, optimizer, loss_fn, epoch):\n",
    "    mean_loss = []\n",
    "    mean_mAP = []\n",
    "\n",
    "    total_batches = len(train_loader)\n",
    "\n",
    "    # update after 20% of the total batches\n",
    "    display_interval = total_batches // 5\n",
    "\n",
    "    for batch_idx, (x, y) in enumerate(train_loader):\n",
    "        x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        out = model(x)\n",
    "        loss = loss_fn(out, y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        pred_boxes, true_boxes = get_bboxes_training(\n",
    "            out, y, iou_threshold=0.5, threshold=0.4\n",
    "        )\n",
    "        mAP = mean_average_precision(\n",
    "            pred_boxes, true_boxes, iou_threshold=0.5, box_format=\"midpoint\"\n",
    "        )\n",
    "\n",
    "        mean_loss.append(loss.item())\n",
    "        mean_mAP.append(mAP.item())\n",
    "\n",
    "        if batch_idx % display_interval == 0 or batch_idx == total_batches - 1:\n",
    "            print(\n",
    "                f\"Epoch: [{epoch}/{EPOCHS}] \\t Iter: [{batch_idx}/{total_batches}] \\t Loss: {loss.item():.4f} \\t mAP: {mAP.item():.4f}\"\n",
    "            )\n",
    "\n",
    "    avg_loss = sum(mean_loss) / len(mean_loss)\n",
    "    avg_mAP = sum(mean_mAP) / len(mean_mAP)\n",
    "    print(colored(f\"Train \\t loss: {avg_loss:.4f} \\t mAP: {avg_mAP:.4f}\", \"green\"))\n",
    "\n",
    "    return avg_mAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_fn(test_loader, model, loss_fn, epoch):\n",
    "    model.eval()\n",
    "    mean_loss = []\n",
    "    mean_mAP = []\n",
    "\n",
    "    for batch_idx, (x, y) in enumerate(test_loader):\n",
    "        x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "        out = model(x)\n",
    "        loss = loss_fn(out, y)\n",
    "\n",
    "        pred_boxes, true_boxes = get_bboxes_training(\n",
    "            out, y, iou_threshold=0.5, threshold=0.4\n",
    "        )\n",
    "        mAP = mean_average_precision(\n",
    "            pred_boxes, true_boxes, iou_threshold=0.5, box_format=\"midpoint\"\n",
    "        )\n",
    "\n",
    "        mean_loss.append(loss.item())\n",
    "        mean_mAP.append(mAP.item())\n",
    "\n",
    "    avg_loss = sum(mean_loss) / len(mean_loss)\n",
    "    avg_mAP = sum(mean_mAP) / len(mean_mAP)\n",
    "    print(colored(f\"Test \\t loss: {avg_loss:3.10f} \\t mAP: {avg_mAP:3.10f}\", \"yellow\"))\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    return avg_mAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aio2024-homework",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
