{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hongong/anaconda3/envs/aio2024-homework/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to downloaded dataset: /home/hongong/.cache/kagglehub/datasets/andrewmvd/dog-and-cat-detection/versions/1\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "# download dataset\n",
    "data_dir = kagglehub.dataset_download(\"andrewmvd/dog-and-cat-detection\")\n",
    "print(\"Path to downloaded dataset:\", data_dir)\n",
    "\n",
    "# Path to downloaded dataset: /Users/hongong/.cache/kagglehub/datasets/andrewmvd/dog-and-cat-detection/versions/1\n",
    "# Path to downloaded dataset: /home/hongong/.cache/kagglehub/datasets/andrewmvd/dog-and-cat-detection/versions/1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import xml.etree.ElementTree as ET\n",
    "from PIL import Image\n",
    "from torchvision import transforms, models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchvision.models.resnet import ResNet18_Weights, ResNet50_Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "\n",
    "    def count_objects_in_annotation(self, annotation_path):\n",
    "        try:\n",
    "            tree = ET.parse(annotation_path)\n",
    "            root = tree.getroot()\n",
    "            count = 0\n",
    "            for objj in root.findall(\"object\"):\n",
    "                count += 1\n",
    "            return count\n",
    "        except FileNotFoundError:\n",
    "            return 0\n",
    "\n",
    "    def filter_images_with_multiple_objects(self):\n",
    "        valid_image_files = []\n",
    "        for f in os.listdir(self.image_dir):\n",
    "            if os.path.isfile(os.path.join(self.image_dir, f)):\n",
    "                img_name = f\n",
    "                annotation_name = os.path.splitext(img_name)[0] + \".xml\"\n",
    "                annotation_path = os.path.join(self.annotations_dir, annotation_name)\n",
    "\n",
    "                # keep images that have single object\n",
    "                if self.count_objects_in_annotation(annotation_path) == 1:\n",
    "                    valid_image_files.append(img_name)\n",
    "        return valid_image_files\n",
    "\n",
    "    def parse_annotation(self, annotation_path):\n",
    "        tree = ET.parse(annotation_path)\n",
    "        root = tree.getroot()\n",
    "\n",
    "        # get image size for normalization\n",
    "        image_width = int(root.find(\"size/width\").text)\n",
    "        image_height = int(root.find(\"size/height\").text)\n",
    "\n",
    "        label = None\n",
    "        bbox = None\n",
    "        for obj in root.findall(\"object\"):\n",
    "            name = obj.find(\"name\").text\n",
    "\n",
    "            # take the first label\n",
    "            if label is None:\n",
    "                label = name\n",
    "\n",
    "                # get bounding box coordinates\n",
    "                xmin = int(obj.find(\"bndbox/xmin\").text)\n",
    "                ymin = int(obj.find(\"bndbox/ymin\").text)\n",
    "                xmax = int(obj.find(\"bndbox/xmax\").text)\n",
    "                ymax = int(obj.find(\"bndbox/ymax\").text)\n",
    "\n",
    "                # normalize bounding box coordinates to [0, 1]\n",
    "                bbox = (\n",
    "                    xmin / image_width,\n",
    "                    ymin / image_height,\n",
    "                    xmax / image_width,\n",
    "                    ymax / image_height,\n",
    "                )\n",
    "\n",
    "        # convert label to numerical representation (0: cat, 1: dog)\n",
    "        label_num = 0 if label == \"cat\" else 1 if label == \"dog\" else -1\n",
    "\n",
    "        return label_num, torch.tensor(bbox, dtype=torch.float32)\n",
    "\n",
    "    def __init__(self, annotations_dir, image_dir, transforms=None):\n",
    "        self.annotations_dir = annotations_dir\n",
    "        self.image_dir = image_dir\n",
    "        self.transforms = transforms\n",
    "        self.image_files = self.filter_images_with_multiple_objects()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # image path\n",
    "        img1_file = self.image_files[idx]\n",
    "        img1_path = os.path.join(self.image_dir, img1_file)\n",
    "\n",
    "        idx2 = np.random.randint(0, len(self.image_files) - 1)\n",
    "        img2_file = self.image_files[idx2]\n",
    "        img2_path = os.path.join(self.image_dir, img2_file)\n",
    "\n",
    "        # annotation path\n",
    "        annotation_name1 = os.path.splitext(img1_file)[0] + \".xml\"\n",
    "        img1_annotations = self.parse_annotation(\n",
    "            os.path.join(self.annotations_dir, annotation_name1)\n",
    "        )\n",
    "\n",
    "        annotation_name2 = os.path.splitext(img2_file)[0] + \".xml\"\n",
    "        img2_annotations = self.parse_annotation(\n",
    "            os.path.join(self.annotations_dir, annotation_name2)\n",
    "        )\n",
    "\n",
    "        # load image\n",
    "        img1 = Image.open(img1_path).convert(\"RGB\")\n",
    "        img2 = Image.open(img2_path).convert(\"RGB\")\n",
    "\n",
    "        # horizontal merge\n",
    "        merged_image = Image.new(\n",
    "            \"RGB\", (img1.width + img2.width, max(img1.height, img2.height))\n",
    "        )\n",
    "        merged_image.paste(img1, (0, 0))\n",
    "        merged_image.paste(img2, (img1.width, 0))\n",
    "        merged_w = img1.width + img2.width\n",
    "        merged_h = max(img1.height, img2.height)\n",
    "\n",
    "        merged_annotations = []\n",
    "\n",
    "        # no change for objects from img1, already normalized\n",
    "        merged_annotations.append(\n",
    "            {\"bbox\": img1_annotations[1].tolist(), \"label\": img1_annotations[0]}\n",
    "        )\n",
    "\n",
    "        # adjust bbox coordinates for objects from img2 AND normalize\n",
    "        new_bbox = [\n",
    "            # normalize xmin\n",
    "            (img2_annotations[1][0] * img2.width + img1.width) / merged_w,\n",
    "            # normalize ymin\n",
    "            img2_annotations[1][1] * img2.height / merged_h,\n",
    "            # normalize xmax\n",
    "            (img2_annotations[1][2] * img2.width + img1.width) / merged_w,\n",
    "            # normalize ymax\n",
    "            img2_annotations[1][3] * img2.height / merged_h,\n",
    "        ]\n",
    "        merged_annotations.append({\"bbox\": new_bbox, \"label\": img2_annotations[0]})\n",
    "\n",
    "        # convert merged image to tensor\n",
    "        if self.transforms:\n",
    "            merged_image = self.transforms(merged_image)\n",
    "        else:\n",
    "            merged_image = transforms.ToTensor()(merged_image)\n",
    "\n",
    "        # convert annotations to 1D tensor, with shape (4,) for bbox and (1,)  for label\n",
    "        annotations = torch.zeros((len(merged_annotations), 5))\n",
    "        for i, ann in enumerate(merged_annotations):\n",
    "            annotations[i] = torch.cat(\n",
    "                (torch.tensor(ann[\"bbox\"]), torch.tensor([ann[\"label\"]]))\n",
    "            )\n",
    "\n",
    "        return merged_image, annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data directory\n",
    "annotations_dir = os.path.join(data_dir, \"annotations\")\n",
    "image_dir = os.path.join(data_dir, \"images\")\n",
    "\n",
    "# Define transformations\n",
    "trans = transforms.Compose([transforms.Resize((224, 224)), transforms.ToTensor()])\n",
    "\n",
    "# Create dataset and dataloaders\n",
    "dataset = MyDataset(annotations_dir, image_dir, transforms=trans)\n",
    "train_dataset, val_dataset = train_test_split(dataset, test_size=0.2, random_state=42)\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleYOLO(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(SimpleYOLO, self).__init__()\n",
    "        self.backbone = models.resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        # Remove the final classification layer of ResNet\n",
    "        self.backbone = nn.Sequential(*list(self.backbone.children())[:-2])\n",
    "        # Add the YOLO head\n",
    "        self.fcs = nn.Linear(\n",
    "            2048, 2 * 2 * (4 + self.num_classes)\n",
    "        )  # 2 is for the number of grid cell\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, C, H, W)\n",
    "        features = self.backbone(x)\n",
    "        features = nn.functional.adaptive_avg_pool2d(\n",
    "            features, (1, 1)\n",
    "        )  # shape: (batch_size, 2048, 1, 1)\n",
    "        features = features.view(features.size(0), -1)  # shape: (batch_size, 2048)\n",
    "        features = self.fcs(features)\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet50-11ad3fa6.pth\" to /home/hongong/.cache/torch/hub/checkpoints/resnet50-11ad3fa6.pth\n"
     ]
    }
   ],
   "source": [
    "# Initialize model, criterion, and optimizer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "num_classes = 2  # Assuming two classes: dog and cat\n",
    "class_to_idx = {\"dog\": 0, \"cat\": 1}\n",
    "\n",
    "model = SimpleYOLO(num_classes).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aio2024-homework",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
