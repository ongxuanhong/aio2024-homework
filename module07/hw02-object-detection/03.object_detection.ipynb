{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/aio2024-homework/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to downloaded dataset: /Users/hongong/.cache/kagglehub/datasets/andrewmvd/dog-and-cat-detection/versions/1\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "# download dataset\n",
    "data_dir = kagglehub.dataset_download(\"andrewmvd/dog-and-cat-detection\")\n",
    "print(\"Path to downloaded dataset:\", data_dir)\n",
    "\n",
    "# Path to downloaded dataset: /Users/hongong/.cache/kagglehub/datasets/andrewmvd/dog-and-cat-detection/versions/1\n",
    "# Path to downloaded dataset: /home/hongong/.cache/kagglehub/datasets/andrewmvd/dog-and-cat-detection/versions/1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import patches\n",
    "import xml.etree.ElementTree as ET\n",
    "from PIL import Image\n",
    "from torchvision import transforms, models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchvision.models.resnet import ResNet18_Weights, ResNet50_Weights\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "\n",
    "    def count_objects_in_annotation(self, annotation_path):\n",
    "        try:\n",
    "            tree = ET.parse(annotation_path)\n",
    "            root = tree.getroot()\n",
    "            count = 0\n",
    "            for objj in root.findall(\"object\"):\n",
    "                count += 1\n",
    "            return count\n",
    "        except FileNotFoundError:\n",
    "            return 0\n",
    "\n",
    "    def filter_images_with_multiple_objects(self):\n",
    "        valid_image_files = []\n",
    "        for f in os.listdir(self.image_dir):\n",
    "            if os.path.isfile(os.path.join(self.image_dir, f)):\n",
    "                img_name = f\n",
    "                annotation_name = os.path.splitext(img_name)[0] + \".xml\"\n",
    "                annotation_path = os.path.join(self.annotations_dir, annotation_name)\n",
    "\n",
    "                # keep images that have single object\n",
    "                if self.count_objects_in_annotation(annotation_path) == 1:\n",
    "                    valid_image_files.append(img_name)\n",
    "        return valid_image_files\n",
    "\n",
    "    def parse_annotation(self, annotation_path):\n",
    "        tree = ET.parse(annotation_path)\n",
    "        root = tree.getroot()\n",
    "\n",
    "        # get image size for normalization\n",
    "        image_width = int(root.find(\"size/width\").text)\n",
    "        image_height = int(root.find(\"size/height\").text)\n",
    "\n",
    "        label = None\n",
    "        bbox = None\n",
    "        for obj in root.findall(\"object\"):\n",
    "            name = obj.find(\"name\").text\n",
    "\n",
    "            # take the first label\n",
    "            if label is None:\n",
    "                label = name\n",
    "\n",
    "                # get bounding box coordinates\n",
    "                xmin = int(obj.find(\"bndbox/xmin\").text)\n",
    "                ymin = int(obj.find(\"bndbox/ymin\").text)\n",
    "                xmax = int(obj.find(\"bndbox/xmax\").text)\n",
    "                ymax = int(obj.find(\"bndbox/ymax\").text)\n",
    "\n",
    "                # normalize bounding box coordinates to [0, 1]\n",
    "                bbox = (\n",
    "                    xmin / image_width,\n",
    "                    ymin / image_height,\n",
    "                    xmax / image_width,\n",
    "                    ymax / image_height,\n",
    "                )\n",
    "\n",
    "        # convert label to numerical representation (0: cat, 1: dog)\n",
    "        label_num = 0 if label == \"cat\" else 1 if label == \"dog\" else -1\n",
    "\n",
    "        return label_num, torch.tensor(bbox, dtype=torch.float32)\n",
    "\n",
    "    def __init__(self, annotations_dir, image_dir, transforms=None):\n",
    "        self.annotations_dir = annotations_dir\n",
    "        self.image_dir = image_dir\n",
    "        self.transforms = transforms\n",
    "        self.image_files = self.filter_images_with_multiple_objects()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # image path\n",
    "        img1_file = self.image_files[idx]\n",
    "        img1_path = os.path.join(self.image_dir, img1_file)\n",
    "\n",
    "        idx2 = np.random.randint(0, len(self.image_files) - 1)\n",
    "        img2_file = self.image_files[idx2]\n",
    "        img2_path = os.path.join(self.image_dir, img2_file)\n",
    "\n",
    "        # annotation path\n",
    "        annotation_name1 = os.path.splitext(img1_file)[0] + \".xml\"\n",
    "        img1_annotations = self.parse_annotation(\n",
    "            os.path.join(self.annotations_dir, annotation_name1)\n",
    "        )\n",
    "\n",
    "        annotation_name2 = os.path.splitext(img2_file)[0] + \".xml\"\n",
    "        img2_annotations = self.parse_annotation(\n",
    "            os.path.join(self.annotations_dir, annotation_name2)\n",
    "        )\n",
    "\n",
    "        # load image\n",
    "        img1 = Image.open(img1_path).convert(\"RGB\")\n",
    "        img2 = Image.open(img2_path).convert(\"RGB\")\n",
    "\n",
    "        # horizontal merge\n",
    "        merged_image = Image.new(\n",
    "            \"RGB\", (img1.width + img2.width, max(img1.height, img2.height))\n",
    "        )\n",
    "        merged_image.paste(img1, (0, 0))\n",
    "        merged_image.paste(img2, (img1.width, 0))\n",
    "        merged_w = img1.width + img2.width\n",
    "        merged_h = max(img1.height, img2.height)\n",
    "\n",
    "        merged_annotations = []\n",
    "\n",
    "        # no change for objects from img1, already normalized\n",
    "        merged_annotations.append(\n",
    "            {\"bbox\": img1_annotations[1].tolist(), \"label\": img1_annotations[0]}\n",
    "        )\n",
    "\n",
    "        # adjust bbox coordinates for objects from img2 AND normalize\n",
    "        new_bbox = [\n",
    "            # normalize xmin\n",
    "            (img2_annotations[1][0] * img2.width + img1.width) / merged_w,\n",
    "            # normalize ymin\n",
    "            img2_annotations[1][1] * img2.height / merged_h,\n",
    "            # normalize xmax\n",
    "            (img2_annotations[1][2] * img2.width + img1.width) / merged_w,\n",
    "            # normalize ymax\n",
    "            img2_annotations[1][3] * img2.height / merged_h,\n",
    "        ]\n",
    "        merged_annotations.append({\"bbox\": new_bbox, \"label\": img2_annotations[0]})\n",
    "\n",
    "        # convert merged image to tensor\n",
    "        if self.transforms:\n",
    "            merged_image = self.transforms(merged_image)\n",
    "        else:\n",
    "            merged_image = transforms.ToTensor()(merged_image)\n",
    "\n",
    "        # convert annotations to 1D tensor, with shape (4,) for bbox and (1,)  for label\n",
    "        annotations = torch.zeros((len(merged_annotations), 5))\n",
    "        for i, ann in enumerate(merged_annotations):\n",
    "            annotations[i] = torch.cat(\n",
    "                (torch.tensor(ann[\"bbox\"]), torch.tensor([ann[\"label\"]]))\n",
    "            )\n",
    "\n",
    "        return merged_image, annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data directory\n",
    "annotations_dir = os.path.join(data_dir, \"annotations\")\n",
    "image_dir = os.path.join(data_dir, \"images\")\n",
    "\n",
    "# Define transformations\n",
    "trans = transforms.Compose([transforms.Resize((224, 224)), transforms.ToTensor()])\n",
    "\n",
    "# Create dataset and dataloaders\n",
    "dataset = MyDataset(annotations_dir, image_dir, transforms=trans)\n",
    "train_dataset, val_dataset = train_test_split(dataset, test_size=0.2, random_state=42)\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleYOLO(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(SimpleYOLO, self).__init__()\n",
    "        self.backbone = models.resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        # Remove the final classification layer of ResNet\n",
    "        self.backbone = nn.Sequential(*list(self.backbone.children())[:-2])\n",
    "        # Add the YOLO head\n",
    "        self.fcs = nn.Linear(\n",
    "            2048, 2 * 2 * (4 + self.num_classes)\n",
    "        )  # 2 is for the number of grid cell\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, C, H, W)\n",
    "        features = self.backbone(x)\n",
    "        features = nn.functional.adaptive_avg_pool2d(\n",
    "            features, (1, 1)\n",
    "        )  # shape: (batch_size, 2048, 1, 1)\n",
    "        features = features.view(features.size(0), -1)  # shape: (batch_size, 2048)\n",
    "        features = self.fcs(features)\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet50-11ad3fa6.pth\" to /Users/hongong/.cache/torch/hub/checkpoints/resnet50-11ad3fa6.pth\n",
      "100%|██████████| 97.8M/97.8M [00:03<00:00, 28.8MB/s]\n"
     ]
    }
   ],
   "source": [
    "# Initialize model, criterion, and optimizer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "num_classes = 2  # Assuming two classes: dog and cat\n",
    "class_to_idx = {\"dog\": 0, \"cat\": 1}\n",
    "\n",
    "model = SimpleYOLO(num_classes).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss(output, targets, device, num_classes):\n",
    "    mse_loss = nn.MSELoss()\n",
    "    ce_loss = nn.CrossEntropyLoss()\n",
    "    batch_size = output.shape[0]\n",
    "    total_loss = 0\n",
    "\n",
    "    output = output.view(\n",
    "        batch_size, 2, 2, 4 + num_classes\n",
    "    )  # reshape to (batch_size, grid_y, grid_x, 4 + num_classes)\n",
    "\n",
    "    for i in range(batch_size):  # iterate through each image in the batch\n",
    "        for j in range(len(targets[i])):  # iterate through each object in the image\n",
    "            # determine which grid cell the object's center falls into\n",
    "            # assuming bbox coordinates are normalized to [0, 1]\n",
    "            bbox_center_x = (targets[i][j][0] + targets[i][j][2]) / 2\n",
    "            bbox_center_y = (targets[i][j][1] + targets[i][j][3]) / 2\n",
    "\n",
    "            grid_x = int(\n",
    "                bbox_center_x * 2\n",
    "            )  # multiply by number of grid cells (2 in this case)\n",
    "            grid_y = int(bbox_center_y * 2)\n",
    "\n",
    "            # 1. classification loss for the responsible grid cell\n",
    "            # convert label to one-hot encoding\n",
    "            label_one_hot = torch.zeros(num_classes, device=device)\n",
    "            label_one_hot[int(targets[i][j][4])] = 1\n",
    "\n",
    "            # classification loss (using cross-entropy loss)\n",
    "            classification_loss = ce_loss(output[i, grid_y, grid_x, 4:], label_one_hot)\n",
    "\n",
    "            # 2. regression loss for the responsible grid cell\n",
    "            bbox_target = targets[i][j][:4].to(device)\n",
    "            regression_loss = mse_loss(output[i, grid_y, grid_x, :4], bbox_target)\n",
    "\n",
    "            # 3. No object loss for the other grid cell\n",
    "            no_obj_loss = 0\n",
    "            for other_grid_y in range(2):\n",
    "                for other_grid_x in range(2):\n",
    "                    if other_grid_y != grid_y and other_grid_x != grid_x:\n",
    "                        no_obj_loss += mse_loss(\n",
    "                            output[i, other_grid_y, other_grid_x, 4],\n",
    "                            torch.zeros(4, device=device),\n",
    "                        )\n",
    "            total_loss += classification_loss + regression_loss + no_obj_loss\n",
    "\n",
    "    return total_loss / batch_size  # average loss over the batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, data_loader, device, num_classes):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "    with torch.no_grad():\n",
    "        for images, targets in tqdm.tqdm(data_loader, desc=\"Validation\", leave=False):\n",
    "            images = images.to(device)\n",
    "            output = model(images)\n",
    "            total_loss = calculate_loss(output, targets, device, num_classes)\n",
    "            running_loss += total_loss.item()\n",
    "\n",
    "            # Reshape output to (batch_size, grid_y, grid_x, 4 + num_classes)\n",
    "            output = output.view(images.shape[0], 2, 2, 4 + num_classes)\n",
    "\n",
    "            # Collect predictions and targets for mAP calculation\n",
    "            for batch_idx in range(images.shape[0]):\n",
    "                for target in targets[batch_idx]:\n",
    "                    # Determine responsible grid cell\n",
    "                    bbox_center_x = (target[0] + target[2]) / 2\n",
    "                    bbox_center_y = (target[1] + target[3]) / 2\n",
    "                    grid_x = int(bbox_center_x * 2)\n",
    "                    grid_y = int(bbox_center_y * 2)\n",
    "\n",
    "                    # Class prediction (index of max probability)\n",
    "                    prediction = output[batch_idx, grid_y, grid_x, 4:].argmax().item()\n",
    "                    all_predictions.append(prediction)\n",
    "                    all_targets.append(target[4].item())\n",
    "\n",
    "    val_loss = running_loss / len(data_loader)\n",
    "\n",
    "    # Convert lists to tensors for PyTorch's metric functions\n",
    "    all_predictions = torch.tensor(all_predictions, device=device)\n",
    "    all_targets = torch.tensor(all_targets, device=device)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    val_accuracy = (all_predictions == all_targets).float().mean()\n",
    "    return val_loss, val_accuracy.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    model, train_loader, val_loader, optimizer, num_epochs, device, num_classes\n",
    "):\n",
    "    best_val_accuracy = 0.0\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_accuracies = []\n",
    "    val_accuracies = []\n",
    "    for epoch in tqdm.tqdm(range(num_epochs), desc=\"Epochs\"):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for images, targets in tqdm.tqdm(train_loader, desc=\"Batches\", leave=False):\n",
    "            images = images.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(images)\n",
    "            total_loss = calculate_loss(output, targets, device, num_classes)\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += total_loss.item()\n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        train_losses.append(epoch_loss)\n",
    "\n",
    "        # Validation\n",
    "        val_loss, val_accuracy = evaluate_model(model, val_loader, device, num_classes)\n",
    "        val_losses.append(val_loss)\n",
    "        val_accuracies.append(val_accuracy)\n",
    "        print(\n",
    "            f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {epoch_loss:.4f}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}\"\n",
    "        )\n",
    "\n",
    "        # Save the best model\n",
    "        if val_accuracy > best_val_accuracy:\n",
    "            best_val_accuracy = val_accuracy\n",
    "            torch.save(model.state_dict(), \"best_model.pth\")\n",
    "\n",
    "    return train_losses, val_losses, train_accuracies, val_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(model, image_path, transform, device, class_to_idx, threshold=0.5):\n",
    "    model.eval()\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    original_witdh, original_height = image.size\n",
    "\n",
    "    # resize the image to match the input size of the model (e.g. 448x448)\n",
    "    resized_image = image.resize((448, 448))\n",
    "    resized_witdh, resized_height = resized_image.size\n",
    "\n",
    "    # apply the same transformations as we did for the training data\n",
    "    transformed_image = transform(resized_image).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(transformed_image)\n",
    "        output = output.view(1, 2, 2, 4 + len(class_to_idx))  # reshape to 2x2 grid\n",
    "\n",
    "        fig, ax = plt.subplots(1)\n",
    "        ax.axis(\"off\")\n",
    "        ax.imshow(resized_image)  # display resized image\n",
    "\n",
    "        for grid_y in range(2):\n",
    "            for grid_x in range(2):\n",
    "                # get the class prediction and bbox for the current grid\n",
    "                class_pred = output[0, grid_y, grid_x, 4:].argmax().item()\n",
    "                bbox = output[0, grid_y, grid_x, :4].tolist()  # predicted bbox\n",
    "\n",
    "                # confidence (probability) of the class prediction\n",
    "                confidence = torch.softmax(output[0, grid_y, grid_x, 4:], dim=0)[\n",
    "                    class_pred\n",
    "                ].item()\n",
    "\n",
    "                # scale the bbox back to the resized image size\n",
    "                # assuming bbox normalized to [0, 1]\n",
    "                x_min = bbox[0] * (resized_witdh / 2) + grid_x * (resized_witdh / 2)\n",
    "                y_min = bbox[1] * (resized_height / 2) + grid_y * (resized_height / 2)\n",
    "                x_max = bbox[2] * (resized_witdh / 2) + grid_x * (resized_witdh / 2)\n",
    "                y_max = bbox[3] * (resized_height / 2) + grid_y * (resized_height / 2)\n",
    "\n",
    "                # draw the bbox and label on the image if confidence is above the threshold\n",
    "                if confidence > threshold:\n",
    "                    rect = patches.Rectangle(\n",
    "                        (x_min, y_min),\n",
    "                        x_max - x_min,\n",
    "                        y_max - y_min,\n",
    "                        linewidth=1,\n",
    "                        edgecolor=\"r\",\n",
    "                        facecolor=\"none\",\n",
    "                    )\n",
    "                    ax.add_patch(rect)\n",
    "                    plt.text(\n",
    "                        x_min,\n",
    "                        y_min,\n",
    "                        f\"{list(class_to_idx.keys())[class_pred]}: {confidence:.2f}\",\n",
    "                        color=\"white\",\n",
    "                        fontsize=12,\n",
    "                        bbox=dict(facecolor=\"red\", alpha=0.5),\n",
    "                    )\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "train_losses, val_losses, train_accuracies, val_accuracies = train_model(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    optimizer,\n",
    "    num_epochs=5,\n",
    "    device=device,\n",
    "    num_classes=num_classes,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best model\n",
    "model.load_state_dict(torch.load(\"best_model.pth\"))\n",
    "# Inference on a sample image\n",
    "image_path = os.path.join(image_dir, \"cat.100.jpg\")\n",
    "inference(model, image_path, trans, device, class_to_idx, threshold=0.5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aio2024-homework",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
