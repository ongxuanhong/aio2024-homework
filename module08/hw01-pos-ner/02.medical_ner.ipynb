{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List, Dict, Tuple\n",
    "from transformers import AutoTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "import evaluate\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessing_Maccrobat:\n",
    "    def __init__(self, dataset_folder, tokenizer):\n",
    "        self.file_ids = [\n",
    "            f.split(\".\")[0] for f in os.listdir(dataset_folder) if f.endswith(\".txt\")\n",
    "        ]\n",
    "\n",
    "        self.text_files = [f + \".txt\" for f in self.file_ids]\n",
    "        self.anno_files = [f + \".ann\" for f in self.file_ids]\n",
    "\n",
    "        self.num_samples = len(self.file_ids)\n",
    "\n",
    "        self.texts: List[str] = []\n",
    "        for i in range(self.num_samples):\n",
    "            file_path = os.path.join(dataset_folder, self.text_files[i])\n",
    "            with open(file_path, \"r\") as f:\n",
    "                self.texts.append(f.read())\n",
    "\n",
    "        self.tags: List[Dict[str, str]] = []\n",
    "        for i in range(self.num_samples):\n",
    "            file_path = os.path.join(dataset_folder, self.anno_files[i])\n",
    "            with open(file_path, \"r\") as f:\n",
    "                text_bound_ann = [\n",
    "                    t.split(\"\\t\") for t in f.read().split(\"\\n\") if t.startswith(\"T\")\n",
    "                ]\n",
    "                text_bound_lst = []\n",
    "                for text_b in text_bound_ann:\n",
    "                    label = text_b[1].split(\" \")\n",
    "                    try:\n",
    "                        _ = int(label[1])\n",
    "                        _ = int(label[2])\n",
    "                        tag = {\n",
    "                            \"text\": text_b[-1],\n",
    "                            \"label\": label[0],\n",
    "                            \"start\": label[1],\n",
    "                            \"end\": label[2],\n",
    "                        }\n",
    "                        text_bound_lst.append(tag)\n",
    "                    except:\n",
    "                        pass\n",
    "\n",
    "                self.tags.append(text_bound_lst)\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def process(self) -> Tuple[List[List[str]], List[List[str]]]:\n",
    "        input_texts = []\n",
    "        input_labels = []\n",
    "\n",
    "        for idx in range(self.num_samples):\n",
    "            full_text = self.texts[idx]\n",
    "            tags = self.tags[idx]\n",
    "\n",
    "            label_offset = []\n",
    "            continuous_label_offset = []\n",
    "            for tag in tags:\n",
    "                offset = list(range(int(tag[\"start\"]), int(tag[\"end\"]) + 1))\n",
    "                label_offset.append(offset)  # 345\n",
    "                continuous_label_offset.extend(offset)  #  345\n",
    "\n",
    "            all_offset = list(range(len(full_text)))\n",
    "            zero_offset = [\n",
    "                offset for offset in all_offset if offset not in continuous_label_offset\n",
    "            ]\n",
    "            zero_offset = Preprocessing_Maccrobat.find_continuous_ranges(\n",
    "                zero_offset\n",
    "            )  # 012 67\n",
    "\n",
    "            self.tokens = []\n",
    "            self.labels = []\n",
    "            self._merge_offset(full_text, tags, zero_offset, label_offset)\n",
    "            assert len(self.tokens) == len(\n",
    "                self.labels\n",
    "            ), f\"Length of tokens and labels are not equal\"\n",
    "\n",
    "            input_texts.append(self.tokens)\n",
    "            input_labels.append(self.labels)\n",
    "\n",
    "        return input_texts, input_labels\n",
    "\n",
    "    def _merge_offset(self, full_text, tags, zero_offset, label_offset):\n",
    "        # zero: [[0,1,2], [6,7]] label: [[3,4,5]]\n",
    "        i = j = 0\n",
    "        while i < len(zero_offset) and j < len(label_offset):\n",
    "            if zero_offset[i][0] < label_offset[j][0]:\n",
    "                self._add_zero(full_text, zero_offset, i)\n",
    "                i += 1\n",
    "            else:\n",
    "                self._add_label(full_text, label_offset, j, tags)\n",
    "                j += 1\n",
    "\n",
    "        while i < len(zero_offset):\n",
    "            self._add_zero(full_text, zero_offset, i)\n",
    "            i += 1\n",
    "\n",
    "        while j < len(label_offset):\n",
    "            self._add_label(full_text, label_offset, j, tags)\n",
    "            j += 1\n",
    "\n",
    "    def _add_zero(self, full_text, offset, index):\n",
    "        start, *_, end = (\n",
    "            offset[index]\n",
    "            if len(offset[index]) > 1\n",
    "            else (offset[index][0], offset[index][0] + 1)\n",
    "        )\n",
    "        text = full_text[start:end]\n",
    "        text_tokens = self.tokenizer.tokenize(text)\n",
    "\n",
    "        self.tokens.extend(text_tokens)\n",
    "        self.labels.extend([\"O\"] * len(text_tokens))\n",
    "\n",
    "    def _add_label(self, full_text, offset, index, tags):\n",
    "        start, *_, end = (\n",
    "            offset[index]\n",
    "            if len(offset[index]) > 1\n",
    "            else (offset[index][0], offset[index][0] + 1)\n",
    "        )\n",
    "        text = full_text[start:end]\n",
    "        text_tokens = self.tokenizer.tokenize(text)\n",
    "\n",
    "        self.tokens.extend(text_tokens)\n",
    "        self.labels.extend(\n",
    "            [f\"B-{tags[index]['label']}\"]\n",
    "            + [f\"I-{tags[index]['label']}\"] * (len(text_tokens) - 1)\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def build_label2id(tokens: List[List[str]]):\n",
    "        label2id = {}\n",
    "        id_counter = 0\n",
    "        for token in [token for sublist in tokens for token in sublist]:\n",
    "            if token not in label2id:\n",
    "                label2id[token] = id_counter\n",
    "                id_counter += 1\n",
    "        return label2id\n",
    "\n",
    "    @staticmethod\n",
    "    def find_continuous_ranges(data: List[int]):  # [0, 1, 2, 6, 7]\n",
    "        if not data:\n",
    "            return []\n",
    "        ranges = []\n",
    "        start = data[0]  # 0\n",
    "        prev = data[0]  # 0\n",
    "        for number in data[1:]:  # [1, 2, 6, 7]\n",
    "            if number != prev + 1:\n",
    "                ranges.append(list(range(start, prev + 1)))\n",
    "                start = number\n",
    "            prev = number\n",
    "        ranges.append(list(range(start, prev + 1)))\n",
    "        return ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', '70', '-', 'year', '-', 'old', 'man', 'was', 'referred', 'to', 'our', 'hospital', 'for', 'gas', '##tric', 'cancer', 'that', 'was', 'detected', 'during', 'screening', 'by', 'es', '##op', '##ha', '##go', '##gas', '##tro', '##du', '##ode', '##nos', '##co', '##py', '(', 'e', '##g', '##d', ')', 'no', 'significant', 'medical', 'history', 'was', 'identified', ',', 'except', 'd', '##ys', '##uria', 'caused', 'by', 'bladder', 'contraction', 'initial', 'laboratory', 'data', 'showed', 'a', 'serum', 'level', 'of', 'af', '##p', 'of', '32', '.', '3', 'ng', '/', 'ml', '(', 'normal', 'range', ':', '0', '-', '15', 'ng', '/', 'ml', ')', ',', 'but', 'which', 'included', 'other', 'tumor', 'markers', ',', 'such', 'as', ',', 'car', '##cino', '##em', '##bry', '##onic', 'antigen', '(', 'ce', '##a', ')', 'and', 'car', '##bo', '##hy', '##dra', '##te', 'antigen', '19', '-', '9', '(', 'ca', '##19', '-', '9', ')', 'no', 'other', 'abnormal', '##ity', 'e', '##g', '##d', 'revealed', 'a', 'mass', 'ul', '##cer', '##of', '##ung', '##ating', '5', '-', 'cm', 'that', 'was', 'comprised', 'of', 'three', 'sept', '##ate', 'ul', '##cer', '##s', 'in', 'the', 'greater', 'curvature', 'of', 'the', 'gas', '##tric', 'ant', '##rum', 'a', 'path', '##ological', 'examination', 'of', 'end', '##os', '##copic', 'bio', '##psy', 'tissues', 'confirmed', 'the', 'presence', 'of', 'aden', '##oca', '##rc', '##ino', '##ma', 'tubular', 'moderately', 'differentiated', 'subsequent', 'computed', 'tom', '##ography', 'abd', '##omi', '##no', '##pel', '##vic', 'visual', '##ized', 'a', 'mass', 'gas', '##tric', 'with', 'deep', 'ul', '##cera', '##tion', 'in', 'the', 'gas', '##tric', 'ant', '##rum', 'with', 'per', '##iga', '##st', '##ric', 'l', '##ym', '##ph', 'node', 'en', '##lar', '##gement', 'no', 'meta', '##static', 'lesions', 'were', 'observed', 'in', 'the', 'liver', 'lung', 'or', 'per', '##ito', '##ne', '##um', 'and', 'chest', 'radio', '##graphy', 'showed', 'no', 'significant', 'findings', 'gas', '##tre', '##ct', '##omy', 'radical', 'sub', '##to', '##tal', 'with', 'd', '##2', 'l', '##ym', '##ph', 'node', 'di', '##sse', '##ction', 'and', 'gas', '##tro', '##je', '##jun', '##ost', '##omy', 'bill', '##rot', '##h', 'ii', 'were', 'performed', '.', 'gross', '##ly', ',', 'the', 'contained', 'the', 'first', 'was', 'with', 'and', 'and', 'the', 'second', 'was', '(', 'figure', '1', ')', '.', 'microscopic', '##ally', ',', 'of', 'with', 'were', 'observed', 'in', 'the', 'first', 'les', '##ion', '.', 'revealed', 'a', 'and', 'at', 'a', '(', 'figure', '2a', ')', 'and', '(', 'figure', '2', '##b', ')', '.', 'the', 'involved', 'the', 'and', 'was', 'found', '(', 'n', '##1', ')', '.', 'showed', 'for', '(', 'hc', '##g', ')', '(', 'figure', '3a', ')', 'and', 'for', '(', 'figure', '3', '##b', ')', '.', 'these', 'findings', 'confirmed', 'the', 'presence', 'of', 'that', 'contained', 'small', 'f', '##oc', '##i', 'of', 'an', 'the', 'second', 'les', '##ion', 'was', 'moderately', 'differentiated', 'which', '(', 't', '##1', '##b', ')', '.', 'it', 'was', 'close', 'to', ',', 'but', 'distinct', 'from', 'the', 'first', 'les', '##ion', ',', 'which', 'was', 'negative', 'by', 'im', '##mun', '##oh', '##isto', '##chemical', 'stain', '##ing', 'for', 'β', '-', 'hc', '##g', 'and', 'af', '##p', '.', 'the', 'patient', 'had', 'an', 'post', '##oper', '##ative', 'course', 'uneven', '##tf', '##ul', 'and', 'was', 'discharged', 'on', 'post', '##oper', '##ative', 'day', '9', 'two', 'weeks', 'later', 'his', 'hc', '##g', 'level', 'was', '176', 'mi', '##u', '/', 'ml', '(', 'normal', 'range', ':', '0', '-', '10', 'mi', '##u', '/', 'ml', ')', 'and', 'his', 'af', '##p', 'level', 'was', '10', '.', '0', 'ng', '/', 'ml', 'of', 'ad', '##ju', '##vant', 'chemotherapy', 'with', 'cape', '##cit', '##abi', '##ne', 'six', 'cycles', '(', 'x', '##elo', '##da', ';', 'hoffmann', '-', 'la', 'roche', 'inc', '.', ',', 'nut', '##ley', ',', 'nj', ',', 'usa', ')', 'was', 'started', 'at', '2500', 'mg', '/', 'm2', 'per', 'day', 'for', '14', 'd', '/', 'cycle', 'after', 'two', 'cycles', 'his', 'β', '-', 'hc', '##g', 'level', 'had', 'to', '<', '3', 'mi', '##u', '/', 'ml', 'and', 'has', 'since', 'remained', 'at', 'this', 'level', '.', 'no', 'rec', '##ur', '##rence', 'or', 'distant', 'meta', '##sta', '##sis', 'had', 'occurred', 'at', 'his', '4', '-', 'year', 'post', '##oper', '##ative', 'follow', '-', 'up', 'res', '##ect', '##ed', 'specimen', 'lesions', 'double', 'a', '5', '.', '8', 'cm', '×', '3', '.', '2', 'cm', 'ul', '##cer', '##of', '##ung', '##ating', 'mass', 'in', 'the', 'ant', '##rum', 'extensive', 'hem', '##or', '##rh', '##age', 'fi', '##bro', '##sis', 'light', 'gray', 'a', 'nearby', '2', '.', '5', 'cm', '×', '2', '.', '0', 'cm', 'ul', '##cera', '##tive', 'les', '##ion', 'tumor', 'cells', 'bizarre', 'pl', '##eo', '##morphic', 'massive', 'numbers', 'hem', '##or', '##rh', '##age', '(', 'sync', '##yt', '##iot', '##rop', '##ho', '##bla', '##sts', 'and', 'cy', '##to', '##tro', '##ph', '##ob', '##las', '##ts', ')', 'hem', '##ato', '##xy', '##lin', 'and', 'e', '##osi', '##n', '(', 'he', ')', '-', 'stained', 'tissues', 'cy', '##top', '##las', '##m', 'purple', 'bu', '##bbly', 'nuclei', 'mag', '##ni', '##fication', 'of', '40', '×', '100', '×', 'giant', 'tumor', 'proper', 'muscle', 'layer', '(', 't', '##2', '##a', ')', 'meta', '##sta', '##sis', 'in', 'four', 'of', '56', 'regional', 'l', '##ym', '##ph', 'nodes', 'im', '##mun', '##oh', '##isto', '##chemical', 'stain', '##ing', 'positive', 'im', '##mun', '##ore', '##act', '##ivity', 'β', '-', 'human', 'cho', '##rion', '##ic', 'go', '##nad', '##ot', '##rop', '##in', 'af', '##p', 'focal', 'po', '##sit', '##ivity', 'cho', '##rio', '##car', '##cino', '##ma', 'gas', '##tric', 'aden', '##oca', '##rc', '##ino', '##ma', 'tubular', 'aden', '##oca', '##rc', '##ino', '##ma', 'af', '##p', '-', 'producing', 'extended', 'to', 'the', 'sub', '##mu', '##cos', '##al', 'layer', 'declined']\n",
      "['O', 'B-Age', 'I-Age', 'I-Age', 'I-Age', 'I-Age', 'B-Sex', 'O', 'B-Clinical_event', 'O', 'O', 'B-Nonbiological_location', 'O', 'B-Biological_structure', 'I-Biological_structure', 'B-Disease_disorder', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Diagnostic_procedure', 'I-Diagnostic_procedure', 'I-Diagnostic_procedure', 'I-Diagnostic_procedure', 'I-Diagnostic_procedure', 'I-Diagnostic_procedure', 'I-Diagnostic_procedure', 'I-Diagnostic_procedure', 'I-Diagnostic_procedure', 'I-Diagnostic_procedure', 'I-Diagnostic_procedure', 'B-Diagnostic_procedure', 'I-Diagnostic_procedure', 'I-Diagnostic_procedure', 'I-Diagnostic_procedure', 'I-Diagnostic_procedure', 'B-History', 'I-History', 'I-History', 'I-History', 'O', 'O', 'O', 'O', 'B-Sign_symptom', 'I-Sign_symptom', 'I-Sign_symptom', 'B-Detailed_description', 'I-Detailed_description', 'I-Detailed_description', 'I-Detailed_description', 'O', 'B-Diagnostic_procedure', 'O', 'O', 'O', 'B-Diagnostic_procedure', 'I-Diagnostic_procedure', 'I-Diagnostic_procedure', 'I-Diagnostic_procedure', 'I-Diagnostic_procedure', 'O', 'B-Lab_value', 'I-Lab_value', 'I-Lab_value', 'I-Lab_value', 'I-Lab_value', 'I-Lab_value', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Diagnostic_procedure', 'I-Diagnostic_procedure', 'I-Diagnostic_procedure', 'I-Diagnostic_procedure', 'I-Diagnostic_procedure', 'I-Diagnostic_procedure', 'B-Diagnostic_procedure', 'I-Diagnostic_procedure', 'I-Diagnostic_procedure', 'I-Diagnostic_procedure', 'O', 'B-Diagnostic_procedure', 'I-Diagnostic_procedure', 'I-Diagnostic_procedure', 'I-Diagnostic_procedure', 'I-Diagnostic_procedure', 'I-Diagnostic_procedure', 'I-Diagnostic_procedure', 'I-Diagnostic_procedure', 'I-Diagnostic_procedure', 'B-Diagnostic_procedure', 'I-Diagnostic_procedure', 'I-Diagnostic_procedure', 'I-Diagnostic_procedure', 'I-Diagnostic_procedure', 'I-Diagnostic_procedure', 'B-Lab_value', 'I-Lab_value', 'I-Lab_value', 'I-Lab_value', 'B-Coreference', 'I-Coreference', 'I-Coreference', 'O', 'O', 'B-Sign_symptom', 'B-Detailed_description', 'I-Detailed_description', 'I-Detailed_description', 'I-Detailed_description', 'I-Detailed_description', 'B-Distance', 'I-Distance', 'I-Distance', 'O', 'O', 'B-Detailed_description', 'I-Detailed_description', 'I-Detailed_description', 'I-Detailed_description', 'I-Detailed_description', 'I-Detailed_description', 'I-Detailed_description', 'I-Detailed_description', 'B-Biological_structure', 'I-Biological_structure', 'I-Biological_structure', 'I-Biological_structure', 'I-Biological_structure', 'I-Biological_structure', 'I-Biological_structure', 'I-Biological_structure', 'I-Biological_structure', 'I-Biological_structure', 'O', 'B-Diagnostic_procedure', 'I-Diagnostic_procedure', 'I-Diagnostic_procedure', 'O', 'B-Biological_structure', 'I-Biological_structure', 'I-Biological_structure', 'I-Biological_structure', 'I-Biological_structure', 'I-Biological_structure', 'O', 'O', 'O', 'O', 'B-Disease_disorder', 'I-Disease_disorder', 'I-Disease_disorder', 'I-Disease_disorder', 'I-Disease_disorder', 'B-Detailed_description', 'B-Detailed_description', 'I-Detailed_description', 'O', 'B-Diagnostic_procedure', 'I-Diagnostic_procedure', 'I-Diagnostic_procedure', 'B-Biological_structure', 'I-Biological_structure', 'I-Biological_structure', 'I-Biological_structure', 'I-Biological_structure', 'O', 'O', 'O', 'B-Sign_symptom', 'B-Biological_structure', 'I-Biological_structure', 'B-Detailed_description', 'I-Detailed_description', 'I-Detailed_description', 'I-Detailed_description', 'I-Detailed_description', 'B-Biological_structure', 'I-Biological_structure', 'I-Biological_structure', 'I-Biological_structure', 'I-Biological_structure', 'I-Biological_structure', 'B-Detailed_description', 'I-Detailed_description', 'I-Detailed_description', 'I-Detailed_description', 'I-Detailed_description', 'I-Detailed_description', 'I-Detailed_description', 'I-Detailed_description', 'I-Detailed_description', 'I-Detailed_description', 'I-Detailed_description', 'I-Detailed_description', 'O', 'B-Sign_symptom', 'I-Sign_symptom', 'I-Sign_symptom', 'O', 'O', 'O', 'O', 'B-Biological_structure', 'B-Biological_structure', 'O', 'B-Biological_structure', 'I-Biological_structure', 'I-Biological_structure', 'I-Biological_structure', 'O', 'B-Biological_structure', 'B-Diagnostic_procedure', 'I-Diagnostic_procedure', 'O', 'B-Sign_symptom', 'I-Sign_symptom', 'I-Sign_symptom', 'B-Therapeutic_procedure', 'I-Therapeutic_procedure', 'I-Therapeutic_procedure', 'I-Therapeutic_procedure', 'B-Detailed_description', 'B-Detailed_description', 'I-Detailed_description', 'I-Detailed_description', 'O', 'B-Biological_structure', 'I-Biological_structure', 'I-Biological_structure', 'I-Biological_structure', 'I-Biological_structure', 'I-Biological_structure', 'B-Therapeutic_procedure', 'I-Therapeutic_procedure', 'I-Therapeutic_procedure', 'O', 'B-Therapeutic_procedure', 'I-Therapeutic_procedure', 'I-Therapeutic_procedure', 'I-Therapeutic_procedure', 'I-Therapeutic_procedure', 'I-Therapeutic_procedure', 'B-Detailed_description', 'I-Detailed_description', 'I-Detailed_description', 'I-Detailed_description', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Therapeutic_procedure', 'I-Therapeutic_procedure', 'I-Therapeutic_procedure', 'I-Therapeutic_procedure', 'B-Lab_value', 'I-Lab_value', 'I-Lab_value', 'O', 'O', 'B-Clinical_event', 'O', 'B-Date', 'I-Date', 'I-Date', 'I-Date', 'I-Date', 'B-Date', 'I-Date', 'I-Date', 'O', 'B-Diagnostic_procedure', 'I-Diagnostic_procedure', 'I-Diagnostic_procedure', 'O', 'B-Lab_value', 'I-Lab_value', 'I-Lab_value', 'I-Lab_value', 'I-Lab_value', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Diagnostic_procedure', 'I-Diagnostic_procedure', 'I-Diagnostic_procedure', 'O', 'B-Lab_value', 'I-Lab_value', 'I-Lab_value', 'I-Lab_value', 'I-Lab_value', 'I-Lab_value', 'O', 'B-Medication', 'I-Medication', 'I-Medication', 'I-Medication', 'O', 'B-Medication', 'I-Medication', 'I-Medication', 'I-Medication', 'B-Frequency', 'I-Frequency', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Dosage', 'I-Dosage', 'I-Dosage', 'I-Dosage', 'I-Dosage', 'I-Dosage', 'I-Dosage', 'I-Dosage', 'I-Dosage', 'I-Dosage', 'I-Dosage', 'B-Date', 'I-Date', 'I-Date', 'O', 'B-Diagnostic_procedure', 'I-Diagnostic_procedure', 'I-Diagnostic_procedure', 'I-Diagnostic_procedure', 'I-Diagnostic_procedure', 'O', 'O', 'B-Lab_value', 'I-Lab_value', 'I-Lab_value', 'I-Lab_value', 'I-Lab_value', 'I-Lab_value', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Sign_symptom', 'I-Sign_symptom', 'I-Sign_symptom', 'O', 'B-Sign_symptom', 'I-Sign_symptom', 'I-Sign_symptom', 'I-Sign_symptom', 'O', 'O', 'O', 'O', 'B-Date', 'I-Date', 'I-Date', 'B-Clinical_event', 'I-Clinical_event', 'I-Clinical_event', 'I-Clinical_event', 'I-Clinical_event', 'I-Clinical_event', 'B-Biological_structure', 'I-Biological_structure', 'I-Biological_structure', 'I-Biological_structure', 'B-Sign_symptom', 'B-Lab_value', 'B-Detailed_description', 'I-Detailed_description', 'I-Detailed_description', 'I-Detailed_description', 'I-Detailed_description', 'I-Detailed_description', 'I-Detailed_description', 'I-Detailed_description', 'I-Detailed_description', 'I-Detailed_description', 'I-Detailed_description', 'I-Detailed_description', 'I-Detailed_description', 'I-Detailed_description', 'I-Detailed_description', 'I-Detailed_description', 'I-Detailed_description', 'I-Detailed_description', 'I-Detailed_description', 'I-Detailed_description', 'B-Detailed_description', 'I-Detailed_description', 'I-Detailed_description', 'I-Detailed_description', 'I-Detailed_description', 'B-Detailed_description', 'I-Detailed_description', 'I-Detailed_description', 'B-Color', 'I-Color', 'B-Detailed_description', 'I-Detailed_description', 'I-Detailed_description', 'I-Detailed_description', 'I-Detailed_description', 'I-Detailed_description', 'I-Detailed_description', 'I-Detailed_description', 'I-Detailed_description', 'I-Detailed_description', 'I-Detailed_description', 'I-Detailed_description', 'I-Detailed_description', 'I-Detailed_description', 'I-Detailed_description', 'I-Detailed_description', 'B-Sign_symptom', 'I-Sign_symptom', 'B-Detailed_description', 'B-Detailed_description', 'I-Detailed_description', 'I-Detailed_description', 'B-Lab_value', 'I-Lab_value', 'B-Detailed_description', 'I-Detailed_description', 'I-Detailed_description', 'I-Detailed_description', 'I-Detailed_description', 'I-Detailed_description', 'I-Detailed_description', 'I-Detailed_description', 'I-Detailed_description', 'I-Detailed_description', 'I-Detailed_description', 'I-Detailed_description', 'I-Detailed_description', 'I-Detailed_description', 'I-Detailed_description', 'I-Detailed_description', 'I-Detailed_description', 'I-Detailed_description', 'I-Detailed_description', 'I-Detailed_description', 'I-Detailed_description', 'B-Diagnostic_procedure', 'I-Diagnostic_procedure', 'I-Diagnostic_procedure', 'I-Diagnostic_procedure', 'I-Diagnostic_procedure', 'I-Diagnostic_procedure', 'I-Diagnostic_procedure', 'I-Diagnostic_procedure', 'I-Diagnostic_procedure', 'I-Diagnostic_procedure', 'I-Diagnostic_procedure', 'I-Diagnostic_procedure', 'I-Diagnostic_procedure', 'I-Diagnostic_procedure', 'B-Biological_structure', 'I-Biological_structure', 'I-Biological_structure', 'I-Biological_structure', 'B-Color', 'B-Shape', 'I-Shape', 'B-Biological_structure', 'B-Detailed_description', 'I-Detailed_description', 'I-Detailed_description', 'I-Detailed_description', 'I-Detailed_description', 'I-Detailed_description', 'B-Detailed_description', 'I-Detailed_description', 'B-Lab_value', 'B-Coreference', 'B-Biological_structure', 'I-Biological_structure', 'I-Biological_structure', 'I-Biological_structure', 'I-Biological_structure', 'I-Biological_structure', 'I-Biological_structure', 'I-Biological_structure', 'B-Sign_symptom', 'I-Sign_symptom', 'I-Sign_symptom', 'B-Biological_structure', 'I-Biological_structure', 'I-Biological_structure', 'I-Biological_structure', 'I-Biological_structure', 'I-Biological_structure', 'I-Biological_structure', 'I-Biological_structure', 'I-Biological_structure', 'B-Diagnostic_procedure', 'I-Diagnostic_procedure', 'I-Diagnostic_procedure', 'I-Diagnostic_procedure', 'I-Diagnostic_procedure', 'I-Diagnostic_procedure', 'I-Diagnostic_procedure', 'B-Lab_value', 'I-Lab_value', 'I-Lab_value', 'I-Lab_value', 'I-Lab_value', 'I-Lab_value', 'B-Diagnostic_procedure', 'I-Diagnostic_procedure', 'I-Diagnostic_procedure', 'I-Diagnostic_procedure', 'I-Diagnostic_procedure', 'I-Diagnostic_procedure', 'I-Diagnostic_procedure', 'I-Diagnostic_procedure', 'I-Diagnostic_procedure', 'I-Diagnostic_procedure', 'I-Diagnostic_procedure', 'B-Diagnostic_procedure', 'I-Diagnostic_procedure', 'B-Lab_value', 'I-Lab_value', 'I-Lab_value', 'I-Lab_value', 'B-Disease_disorder', 'I-Disease_disorder', 'I-Disease_disorder', 'I-Disease_disorder', 'I-Disease_disorder', 'B-Biological_structure', 'I-Biological_structure', 'B-Disease_disorder', 'I-Disease_disorder', 'I-Disease_disorder', 'I-Disease_disorder', 'I-Disease_disorder', 'B-Biological_structure', 'B-Disease_disorder', 'I-Disease_disorder', 'I-Disease_disorder', 'I-Disease_disorder', 'I-Disease_disorder', 'B-Detailed_description', 'I-Detailed_description', 'I-Detailed_description', 'I-Detailed_description', 'B-Biological_structure', 'I-Biological_structure', 'I-Biological_structure', 'I-Biological_structure', 'I-Biological_structure', 'I-Biological_structure', 'I-Biological_structure', 'I-Biological_structure', 'B-Lab_value']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"d4data/biomedical-ner-all\")\n",
    "\n",
    "dataset_folder = \"./MACCROBAT2018\"\n",
    "\n",
    "Maccrobat_builder = Preprocessing_Maccrobat(dataset_folder, tokenizer)\n",
    "input_texts, input_labels = Maccrobat_builder.process()\n",
    "\n",
    "# print\n",
    "print(input_texts[0])\n",
    "print(input_labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'O': 0, 'B-Age': 1, 'I-Age': 2, 'B-Sex': 3, 'B-Clinical_event': 4, 'B-Nonbiological_location': 5, 'B-Biological_structure': 6, 'I-Biological_structure': 7, 'B-Disease_disorder': 8, 'B-Diagnostic_procedure': 9, 'I-Diagnostic_procedure': 10, 'B-History': 11, 'I-History': 12, 'B-Sign_symptom': 13, 'I-Sign_symptom': 14, 'B-Detailed_description': 15, 'I-Detailed_description': 16, 'B-Lab_value': 17, 'I-Lab_value': 18, 'B-Coreference': 19, 'I-Coreference': 20, 'B-Distance': 21, 'I-Distance': 22, 'I-Disease_disorder': 23, 'B-Therapeutic_procedure': 24, 'I-Therapeutic_procedure': 25, 'B-Date': 26, 'I-Date': 27, 'B-Medication': 28, 'I-Medication': 29, 'B-Frequency': 30, 'I-Frequency': 31, 'B-Dosage': 32, 'I-Dosage': 33, 'I-Clinical_event': 34, 'B-Color': 35, 'I-Color': 36, 'B-Shape': 37, 'I-Shape': 38, 'B-Severity': 39, 'B-Duration': 40, 'I-Duration': 41, 'B-Administration': 42, 'I-Administration': 43, 'B-Personal_background': 44, 'B-Activity': 45, 'I-Activity': 46, 'B-Outcome': 47, 'I-Nonbiological_location': 48, 'I-Severity': 49, 'B-Area': 50, 'I-Area': 51, 'B-Quantitative_concept': 52, 'B-Volume': 53, 'I-Volume': 54, 'B-Family_history': 55, 'I-Family_history': 56, 'B-Texture': 57, 'I-Texture': 58, 'I-Quantitative_concept': 59, 'I-Personal_background': 60, 'B-Subject': 61, 'I-Subject': 62, 'B-Time': 63, 'I-Time': 64, 'I-Outcome': 65, 'B-Height': 66, 'I-Height': 67, 'B-Occupation': 68, 'I-Occupation': 69, 'B-Other_entity': 70, 'I-Other_entity': 71, 'B-Other_event': 72, 'I-Other_event': 73, 'B-Weight': 74, 'I-Weight': 75, 'B-Qualitative_concept': 76, 'I-Qualitative_concept': 77, 'B-Biological_attribute': 78, 'I-Biological_attribute': 79, 'B-Mass': 80, 'I-Mass': 81, 'I-Sex': 82}\n",
      "{0: 'O', 1: 'B-Age', 2: 'I-Age', 3: 'B-Sex', 4: 'B-Clinical_event', 5: 'B-Nonbiological_location', 6: 'B-Biological_structure', 7: 'I-Biological_structure', 8: 'B-Disease_disorder', 9: 'B-Diagnostic_procedure', 10: 'I-Diagnostic_procedure', 11: 'B-History', 12: 'I-History', 13: 'B-Sign_symptom', 14: 'I-Sign_symptom', 15: 'B-Detailed_description', 16: 'I-Detailed_description', 17: 'B-Lab_value', 18: 'I-Lab_value', 19: 'B-Coreference', 20: 'I-Coreference', 21: 'B-Distance', 22: 'I-Distance', 23: 'I-Disease_disorder', 24: 'B-Therapeutic_procedure', 25: 'I-Therapeutic_procedure', 26: 'B-Date', 27: 'I-Date', 28: 'B-Medication', 29: 'I-Medication', 30: 'B-Frequency', 31: 'I-Frequency', 32: 'B-Dosage', 33: 'I-Dosage', 34: 'I-Clinical_event', 35: 'B-Color', 36: 'I-Color', 37: 'B-Shape', 38: 'I-Shape', 39: 'B-Severity', 40: 'B-Duration', 41: 'I-Duration', 42: 'B-Administration', 43: 'I-Administration', 44: 'B-Personal_background', 45: 'B-Activity', 46: 'I-Activity', 47: 'B-Outcome', 48: 'I-Nonbiological_location', 49: 'I-Severity', 50: 'B-Area', 51: 'I-Area', 52: 'B-Quantitative_concept', 53: 'B-Volume', 54: 'I-Volume', 55: 'B-Family_history', 56: 'I-Family_history', 57: 'B-Texture', 58: 'I-Texture', 59: 'I-Quantitative_concept', 60: 'I-Personal_background', 61: 'B-Subject', 62: 'I-Subject', 63: 'B-Time', 64: 'I-Time', 65: 'I-Outcome', 66: 'B-Height', 67: 'I-Height', 68: 'B-Occupation', 69: 'I-Occupation', 70: 'B-Other_entity', 71: 'I-Other_entity', 72: 'B-Other_event', 73: 'I-Other_event', 74: 'B-Weight', 75: 'I-Weight', 76: 'B-Qualitative_concept', 77: 'I-Qualitative_concept', 78: 'B-Biological_attribute', 79: 'I-Biological_attribute', 80: 'B-Mass', 81: 'I-Mass', 82: 'I-Sex'}\n"
     ]
    }
   ],
   "source": [
    "label2id = Preprocessing_Maccrobat.build_label2id(input_labels)\n",
    "id2label = {v: k for k, v in label2id.items()}\n",
    "\n",
    "# print\n",
    "print(label2id)\n",
    "print(id2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160 40 160 40\n"
     ]
    }
   ],
   "source": [
    "inputs_train, inputs_val, labels_train, labels_val = train_test_split(\n",
    "    input_texts, input_labels, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(len(inputs_train), len(inputs_val), len(labels_train), len(labels_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 512\n",
    "\n",
    "\n",
    "class NER_Dataset(Dataset):\n",
    "    def __init__(self, input_texts, input_labels, tokenizer, label2id, max_len=MAX_LEN):\n",
    "        super().__init__()\n",
    "        self.tokens = input_texts\n",
    "        self.labels = input_labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.label2id = label2id\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokens)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_token = self.tokens[idx]\n",
    "        label_token = [self.label2id[label] for label in self.labels[idx]]\n",
    "\n",
    "        input_token = self.tokenizer.convert_tokens_to_ids(input_token)\n",
    "        attention_mask = [1] * len(input_token)\n",
    "\n",
    "        input_ids = self.pad_and_truncate(\n",
    "            input_token, pad_id=self.tokenizer.pad_token_id\n",
    "        )\n",
    "        labels = self.pad_and_truncate(label_token, pad_id=0)\n",
    "        attention_mask = self.pad_and_truncate(attention_mask, pad_id=0)\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": torch.as_tensor(input_ids),\n",
    "            \"labels\": torch.as_tensor(labels),\n",
    "            \"attention_mask\": torch.as_tensor(attention_mask),\n",
    "        }\n",
    "\n",
    "    def pad_and_truncate(self, inputs: List[int], pad_id: int):\n",
    "        if len(inputs) < self.max_len:\n",
    "            padded_inputs = inputs + [pad_id] * (self.max_len - len(inputs))\n",
    "        else:\n",
    "            padded_inputs = inputs[: self.max_len]\n",
    "        return padded_inputs\n",
    "\n",
    "    def label2id(self, labels: List[str]):\n",
    "        return [self.label2id[label] for label in labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = NER_Dataset(inputs_train, labels_train, tokenizer, label2id)\n",
    "val_set = NER_Dataset(inputs_val, labels_val, tokenizer, label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1999,  2254,  2268,  1037,  5401,  1011,  2095,  1011,  2214,  2450,\n",
      "        12636, 20630,  3591,  2007,  2460,  2791,  1997,  3052,  2016,  2018,\n",
      "         1037,  1015,  1011,  3204,  2381,  1997,  2460,  2791,  1997,  3052,\n",
      "         6555,  1998,  1037,  1999,  6912,  3977, 16612,  9885,  3905,  2000,\n",
      "         1040,  7274,  2361, 22084, 10256,  2016,  2988,  2053,  3176,  8030,\n",
      "         2012,  2287,  2871,  2016,  2018,  2042, 11441,  2007,  1037,  7388,\n",
      "         4456,  2187,  1011, 11536,  1056,  2487, 24700,  2487,  2754,  2462,\n",
      "         2050,  2754,  2462,  2050,  1010,  1056,  2487, 24700,  2487,  1010,\n",
      "         2187,  1011, 11536,  7388,  4456,  3988,  3949,  2018,  2443,  1037,\n",
      "        15116, 22471, 16940,  1998, 13045,  4487, 11393,  7542, 22260,  9386,\n",
      "         2854,  2016,  3525,  9601,  1997,  2079,  2595,  7242,  1006,  4293,\n",
      "        11460,  1013, 25525,  1018, 12709,  1010,  2628,  2011,  1022, 12709,\n",
      "         1997, 22330, 20464,  7361, 15006, 21890, 24284,  2777, 12326,  2890,\n",
      "        18684,  2618,  1998,  1019,  1011, 19857, 14604,  4648,  6895,  2140,\n",
      "         4800, 11644,  7654, 27404,  2077,  1998,  2044, 27144,  3662, 15050,\n",
      "         3853,  3671,  2044, 27144,  2016,  9601,  8249,  2187,  2878,  1011,\n",
      "         7388,  2007,  2019, 22260,  9386,  2854, 12992,  2138,  1996, 13656,\n",
      "         2018,  2042,  9765, 22991, 10769,  1011,  3893,  2014,  4745,  2966,\n",
      "         6939,  2078,  5031,  2069,  1997,  3424,  1011,  9765, 22991,  7242,\n",
      "         2016,  2165, 17214, 11636, 29323,  2078,  2005,  1019,  2086,  1998,\n",
      "         1010,  2412,  2144,  1010,  1996, 23958, 18260, 24054,  2292,  3217,\n",
      "         6844,  2571,  1999,  1996,  2459,  2086,  2044, 27144,  1010,  2016,\n",
      "         2018,  2042,  3161,  1998,  1999,  2740,  4659,  2204,  1999,  2804,\n",
      "         2000,  2014,  2060,  8030,  1010,  2016,  2085,  3591,  2007, 11937,\n",
      "        11714, 11522,  2401, 11937, 11714,  2361, 22084,  1998, 23760, 29048,\n",
      "         2016,  2018,  4417,  4487, 16173,  3508, 26536,  7934,  2310, 18674,\n",
      "         2019,  1055,  2509, 21908, 10958,  4244,  1998,  3968, 14545, 15965,\n",
      "         7637,  3988,  5911,  5300,  2020,  2306,  3671,  6537,  3272,  2005,\n",
      "         2019,  2504,  1997,  1050,  1011,  5536,  4013,  1011,  4167, 14085,\n",
      "         3089,  5397,  4588, 25117,  1006,  1028,  1016,  1010,  2199, 18720,\n",
      "         1013, 19875,  8319,  1012,  3463,  1997,  4812,  2046,  1996,  2047,\n",
      "         1011, 14447,  4003, 18994,  7677, 20166,  2443, 15050,  9007,  3671,\n",
      "         3798,  1010,  2019, 16175, 11522,  3695, 13113,  1006, 14925,  2290,\n",
      "         2008,  3936,  2053,  2003,  5403,  7712,  3431,  1998,  1037, 17076,\n",
      "         3695, 13113, 21887,  2854,  1997,  3671,  3311,  1012,  1996, 14925,\n",
      "         2290,  3662, 11937, 11714, 11522,  2401,  8254,  2271,  2007,  2187,\n",
      "         1011,  8123, 24353, 21371, 18834,  7277,  7934, 15420,  6976,  4372,\n",
      "         8017, 20511,  2187,  2012, 14482,  1998,  1053,  2869, 15420,  2659,\n",
      "         1011, 10004,  2007,  2358,  3431,  2512, 13102,  8586, 18513,  1006,\n",
      "        20965,  1012,  1015,  1007,  1012,  1037,  9052, 11522,  3695, 13113,\n",
      "         1016,  1011,  8789,  3936,  1037,  2187, 18834,  7277,  7934,  1041,\n",
      "        20614,  3258, 12884,  1006,  1048,  3726,  2546,  1997,  1014,  1012,\n",
      "         2322,  1006,  1044, 22571, 23212,  5267,  2483,  1998,  1037, 29454,\n",
      "         4383,  5729,  2187, 18834,  7277,  7934, 28105,  1048,  2615, 19499,\n",
      "         2187, 26204,  2000,  2488,  9375,  1996,  3426,  1997,  1996,  1048,\n",
      "         2615, 25353, 16033, 10415, 28466,  1010, 22935,  8060, 17011,  1006,\n",
      "         4642,  2099,  2001,  2864,  1012,  2009,  4484,  1996,  1048,  3726,\n",
      "         2546,  1997,  1014,  1012,  2322,  1996,  1056,  2475,  1011, 18215,\n",
      "         5537,  3662,  3905,  2000, 28466,  4834,  4030,  1048,  2615,  1998,\n",
      "         2053,  3968, 14545,  2026, 24755, 25070,  1006, 20965,  1012, 23409,\n",
      "         1007,  1012, 11721,  3527, 22153,  2819, 22415,  2397, 21362,  4857,\n",
      "         5582,  2026])\n",
      "tensor([ 0, 26, 27,  0,  1,  2,  2,  2,  2,  3,  0,  0,  4,  0, 13, 14, 14, 14,\n",
      "         0,  0,  0, 40, 41, 41,  0,  0, 13, 14, 14, 14, 15,  0,  0,  0,  9, 10,\n",
      "        17, 18,  0,  0, 13, 14, 14, 14, 39,  0,  0,  0, 13, 14,  0, 26, 27,  0,\n",
      "         0,  0,  4,  0,  0,  8, 23, 15, 16, 16, 17, 18, 18, 18, 17, 18, 18, 11,\n",
      "        12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,  0,  0,  0,  0,  0,\n",
      "        24, 25, 25,  0, 24, 25, 25, 25,  6,  7,  7,  0,  0,  0,  0, 28, 29, 29,\n",
      "         0, 32, 33, 33, 33, 32, 33,  0,  0,  0, 32, 33,  0, 28, 29, 29, 29, 29,\n",
      "        29, 28, 29, 29, 29, 29,  0, 28, 29, 29, 29, 29, 29, 29,  9, 10, 10, 10,\n",
      "         0,  0,  0, 28,  0,  9, 10, 17,  0, 28,  0,  0, 24,  6,  7,  7,  7,  0,\n",
      "         0, 15, 16, 16, 16,  0,  0, 13,  0,  0, 15, 16, 16, 16, 16,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0, 28, 29, 29, 29, 29,  0,  0, 28, 29, 29, 29,  0, 40,\n",
      "        41,  0,  0,  0,  0,  0,  0,  0,  0,  0, 28, 29, 29, 29,  0,  0, 40, 41,\n",
      "        41,  0,  0,  0,  0,  0, 13,  0,  0,  9, 17, 18,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0, 13, 14, 14, 14, 13, 14, 14, 14,  0, 13, 14,  0,  0,\n",
      "         0, 13, 14, 14,  6,  7,  7,  7,  0, 13, 14, 13, 14, 14,  0, 13, 14, 15,\n",
      "        39,  0,  9, 10,  0, 17, 18, 18,  0,  0,  0,  0,  0,  9, 10, 10, 10, 10,\n",
      "        10, 10, 10, 10, 10, 10,  0, 17, 18, 18, 18, 18, 18, 18, 17,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  8, 23, 23, 23,  0,  9, 10, 17,  0,  0,  0,  9,\n",
      "        10, 10, 10,  0,  9, 10,  0,  0,  0, 13, 14, 14, 14,  0,  0,  9, 10, 10,\n",
      "         6,  7,  0, 17,  0,  0,  0,  9, 10,  0, 13, 14, 14, 14, 15, 16,  0, 13,\n",
      "        14, 14, 14, 13, 14, 14, 14, 14, 30, 13, 14, 14,  6,  7,  7,  0, 13, 14,\n",
      "        14, 15, 16, 16,  0, 13, 14, 15, 16, 16, 16,  0,  0,  0,  0,  0,  0,  0,\n",
      "         9, 10, 10, 10, 15, 16, 16,  0,  0,  9, 10, 10, 10, 10, 10, 10, 10,  0,\n",
      "         9, 10, 10,  0, 17, 18, 18,  0, 13, 14, 14, 14, 14,  0,  0, 13, 14, 39,\n",
      "         6,  7,  7,  7, 15,  6,  7, 39,  6,  7,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  9, 10, 10,  0,  9, 10,  0,  0,  0,  0,  0,  0,\n",
      "         9, 10, 10,  0, 17, 18, 18,  0,  9, 10, 10, 10, 10,  0,  0,  0,  8,  9,\n",
      "        17,  6,  7,  0,  0, 13, 14,  6,  7,  7,  0,  0,  0,  0,  0,  0,  9, 10,\n",
      "        10, 10, 10, 15,  0, 13, 14,  6])\n"
     ]
    }
   ],
   "source": [
    "# print\n",
    "print(train_set[0][\"input_ids\"])\n",
    "print(train_set[0][\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at d4data/biomedical-ner-all and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([84]) in the checkpoint and torch.Size([83]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([84, 768]) in the checkpoint and torch.Size([83, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DistilBertForTokenClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): DistilBertSdpaAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=83, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    \"d4data/biomedical-ner-all\",\n",
    "    label2id=label2id,\n",
    "    id2label=id2label,\n",
    "    ignore_mismatched_sizes=True,\n",
    ")\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = evaluate.load(\"accuracy\")\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    mask = labels != 0\n",
    "    predictions = np.argmax(predictions, axis=-1)\n",
    "    return accuracy.compute(predictions=predictions[mask], references=labels[mask])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3534/2873659598.py:14: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"ner-biomedical-maccrobat2018\",\n",
    "    learning_rate=1e-4,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=20,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    optim=\"adamw_torch\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_set,\n",
    "    eval_dataset=val_set,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='200' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [200/200 02:45, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.774000</td>\n",
       "      <td>1.695740</td>\n",
       "      <td>0.337673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.447200</td>\n",
       "      <td>0.984300</td>\n",
       "      <td>0.591136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.914300</td>\n",
       "      <td>0.718306</td>\n",
       "      <td>0.703509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.650700</td>\n",
       "      <td>0.618407</td>\n",
       "      <td>0.752539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.501700</td>\n",
       "      <td>0.577020</td>\n",
       "      <td>0.763989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.403400</td>\n",
       "      <td>0.552041</td>\n",
       "      <td>0.784118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.326800</td>\n",
       "      <td>0.536559</td>\n",
       "      <td>0.785596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.271000</td>\n",
       "      <td>0.539278</td>\n",
       "      <td>0.786057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.225900</td>\n",
       "      <td>0.542826</td>\n",
       "      <td>0.793536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.192000</td>\n",
       "      <td>0.539999</td>\n",
       "      <td>0.796953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.166500</td>\n",
       "      <td>0.547726</td>\n",
       "      <td>0.798615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.146100</td>\n",
       "      <td>0.550189</td>\n",
       "      <td>0.801570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.128000</td>\n",
       "      <td>0.561947</td>\n",
       "      <td>0.801108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.118900</td>\n",
       "      <td>0.559150</td>\n",
       "      <td>0.805078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.107600</td>\n",
       "      <td>0.561455</td>\n",
       "      <td>0.802216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.101100</td>\n",
       "      <td>0.558707</td>\n",
       "      <td>0.803878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.095900</td>\n",
       "      <td>0.565571</td>\n",
       "      <td>0.802308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.090300</td>\n",
       "      <td>0.568967</td>\n",
       "      <td>0.802955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.089300</td>\n",
       "      <td>0.568564</td>\n",
       "      <td>0.802493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.086200</td>\n",
       "      <td>0.568380</td>\n",
       "      <td>0.801847</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 37s, sys: 8.4 s, total: 2min 46s\n",
      "Wall time: 2min 46s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=200, training_loss=0.4418525362014771, metrics={'train_runtime': 166.2238, 'train_samples_per_second': 19.251, 'train_steps_per_second': 1.203, 'total_flos': 418702245888000.0, 'train_loss': 0.4418525362014771, 'epoch': 20.0})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.push_to_hub(\n",
    "#     commit_message=\"Training complete\",\n",
    "#     token=\"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\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A\tO\n",
      "48\tB-Lab_value\n",
      "year-old\tI-Age\n",
      "female\tB-Sex\n",
      "presented\tO\n",
      "with\tO\n",
      "vaginal\tO\n",
      "bleeding\tB-Sign_symptom\n",
      "and\tO\n",
      "abnormal\tB-Lab_value\n",
      "Pap\tB-Lab_value\n",
      "smears.\tI-Detailed_description\n",
      "Upon\tO\n",
      "diagnosis\tO\n",
      "of\tO\n",
      "invasive\tB-Detailed_description\n",
      "non-keratinizing\tO\n",
      "SCC\tO\n",
      "of\tO\n",
      "the\tO\n",
      "cervix,\tO\n",
      "she\tO\n",
      "underwent\tO\n",
      "a\tO\n",
      "radical\tO\n",
      "hysterectomy\tB-Lab_value\n",
      "with\tO\n",
      "salpingo-oophorectomy\tO\n",
      "which\tO\n",
      "demonstrated\tO\n",
      "positive\tB-Lab_value\n",
      "spread\tI-Lab_value\n",
      "to\tO\n",
      "the\tO\n",
      "pelvic\tB-Biological_structure\n",
      "lymph\tI-Biological_structure\n",
      "nodes\tO\n",
      "and\tO\n",
      "the\tO\n",
      "parametrium.\tB-Diagnostic_procedure\n",
      "Pathological\tI-Diagnostic_procedure\n",
      "examination\tO\n",
      "revealed\tO\n",
      "that\tO\n",
      "the\tO\n",
      "tumour\tO\n",
      "also\tO\n",
      "extensively\tO\n",
      "involved\tO\n",
      "the\tO\n",
      "lower\tO\n",
      "uterine\tI-Detailed_description\n",
      "segment.\tI-Detailed_description\n",
      "CPU times: user 10.8 ms, sys: 13.9 ms, total: 24.7 ms\n",
      "Wall time: 31 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "test_sentence = \"\"\"A 48 year-old female presented with vaginal bleeding and abnormal \n",
    "Pap smears. Upon diagnosis of invasive non-keratinizing SCC of the cervix, she \n",
    "underwent a radical hysterectomy with salpingo-oophorectomy which demonstrated \n",
    "positive spread to the pelvic lymph nodes and the parametrium. Pathological \n",
    "examination revealed that the tumour also extensively involved the lower uterine \n",
    "segment.\n",
    "\"\"\"\n",
    "\n",
    "# tokenization\n",
    "input = torch.as_tensor([tokenizer.convert_tokens_to_ids(test_sentence.split())])\n",
    "input = input.to(\"cuda\")\n",
    "\n",
    "# prediction\n",
    "outputs = model(input)\n",
    "_, preds = torch.max(outputs.logits, -1)\n",
    "preds = preds[0].cpu().numpy()\n",
    "\n",
    "# decode\n",
    "for token, pred in zip(test_sentence.split(), preds):\n",
    "    print(f\"{token}\\t{id2label[pred]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aio2024-homework",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
