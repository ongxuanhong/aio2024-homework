{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "\n",
    "from torchsummary import summary\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import re\n",
    "import string\n",
    "from langid.langid import LanguageIdentifier, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape: (30000, 2)\n",
      "Valid data shape: (10000, 2)\n",
      "Test data shape: (10000, 2)\n"
     ]
    }
   ],
   "source": [
    "def load_data_from_path(folder_path):\n",
    "    examples = []\n",
    "    for label in os.listdir(folder_path):\n",
    "        full_path = os.path.join(folder_path, label)\n",
    "        for file_name in os.listdir(full_path):\n",
    "            file_path = os.path.join(full_path, file_name)\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                lines = f.readlines()\n",
    "                sentence = \" \".join(lines)\n",
    "                if label == \"neg\":\n",
    "                    label = 0\n",
    "                if label == \"pos\":\n",
    "                    label = 1\n",
    "                data = {\"sentence\": sentence, \"label\": label}\n",
    "                examples.append(data)\n",
    "    return pd.DataFrame(examples)\n",
    "\n",
    "\n",
    "folder_paths = {\n",
    "    \"train\": \"data/ntc-scv/data_train/train\",\n",
    "    \"valid\": \"data/ntc-scv/data_train/test\",\n",
    "    \"test\": \"data/ntc-scv/data_test/test\",\n",
    "}\n",
    "\n",
    "train_df = load_data_from_path(folder_paths[\"train\"])\n",
    "valid_df = load_data_from_path(folder_paths[\"valid\"])\n",
    "test_df = load_data_from_path(folder_paths[\"test\"])\n",
    "\n",
    "# show data shape\n",
    "print(\"Train data shape:\", train_df.shape)\n",
    "print(\"Valid data shape:\", valid_df.shape)\n",
    "print(\"Test data shape:\", test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape after removing non-vietnamese sentences: (29736, 2)\n",
      "Train data shape of non-vietnamese sentences: (264, 2)\n"
     ]
    }
   ],
   "source": [
    "def identify_vn(df):\n",
    "    identifier = LanguageIdentifier.from_modelstring(model, norm_probs=True)\n",
    "    not_vi_idx = set()\n",
    "    THRESHOLD = 0.9\n",
    "    for idx, row in df.iterrows():\n",
    "        score = identifier.classify(row[\"sentence\"])\n",
    "        if score[0] != \"vi\" or (score[0] == \"vi\" and score[1] <= THRESHOLD):\n",
    "            not_vi_idx.add(idx)\n",
    "    vi_df = df[~df.index.isin(not_vi_idx)]\n",
    "    not_vi_df = df[df.index.isin(not_vi_idx)]\n",
    "    return vi_df, not_vi_df\n",
    "\n",
    "\n",
    "train_df_vi, train_df_other = identify_vn(train_df)\n",
    "\n",
    "print(\"Train data shape after removing non-vietnamese sentences:\", train_df_vi.shape)\n",
    "print(\"Train data shape of non-vietnamese sentences:\", train_df_other.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    url_pattern = re.compile(r\"https?://\\s+\\wwww\\.\\s+\")\n",
    "    text = url_pattern.sub(r\" \", text)\n",
    "    html_pattern = re.compile(r\"<[^<>]+>\")\n",
    "    text = html_pattern.sub(\" \", text)\n",
    "    replace_chars = list(string.punctuation + string.digits)\n",
    "    for char in replace_chars:\n",
    "        text = text.replace(char, \" \")\n",
    "    emoji_pattern = re.compile(\n",
    "        \"[\"\n",
    "        \"\\U0001F600-\\U0001F64F\"\n",
    "        \"\\U0001F300-\\U0001F5FF\"\n",
    "        \"\\U0001F680-\\U0001F6FF\"\n",
    "        \"\\U0001F1E0-\\U0001F1FF\"\n",
    "        \"\\U0001F1F2-\\U0001F1F4\"\n",
    "        \"\\U0001F1E6-\\U0001F1FF\"\n",
    "        \"\\U0001F600-\\U0001F64F\"\n",
    "        \"\\U00002702-\\U000027B0\"\n",
    "        \"\\U000024C2-\\U0001F251\"\n",
    "        \"\\U0001f926-\\U0001f937\"\n",
    "        \"\\U0001F1F2\"\n",
    "        \"\\U0001F1F4\"\n",
    "        \"\\U0001F620\"\n",
    "        \"\\u200d\"\n",
    "        \"\\u2640-\\u2642\"\n",
    "        \"]+\",\n",
    "        flags=re.UNICODE,\n",
    "    )\n",
    "    text = emoji_pattern.sub(r\" \", text)\n",
    "    text = \" \".join(text.split())\n",
    "    return text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/y2/hzjfncr92llg100m08nwxmhc0000gn/T/ipykernel_37258/315947395.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_df_vi[\"preprocess_sentence\"] = train_df_vi[\"sentence\"].apply(preprocess_text)\n"
     ]
    }
   ],
   "source": [
    "train_df_vi[\"preprocess_sentence\"] = train_df_vi[\"sentence\"].apply(preprocess_text)\n",
    "valid_df[\"preprocess_sentence\"] = valid_df[\"sentence\"].apply(preprocess_text)\n",
    "test_df[\"preprocess_sentence\"] = test_df[\"sentence\"].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word-based tokenizer\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.data.functional import to_map_style_dataset\n",
    "\n",
    "tokenizer = get_tokenizer(\"basic_english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create iter dataset\n",
    "def yield_tokens(sentences, tokenizer):\n",
    "    for sentence in sentences:\n",
    "        yield tokenizer(sentence)\n",
    "\n",
    "\n",
    "# build vocabulary\n",
    "vocab_size = 10000\n",
    "vocabulary = build_vocab_from_iterator(\n",
    "    yield_tokens(train_df_vi[\"preprocess_sentence\"], tokenizer),\n",
    "    max_size=vocab_size,\n",
    "    specials=[\"<pad>\", \"<unk>\"],\n",
    ")\n",
    "vocabulary.set_default_index(vocabulary[\"<unk>\"])\n",
    "\n",
    "\n",
    "# convert iter into torchtext dataset\n",
    "def prepare_dataset(df):\n",
    "    for index, row in df.iterrows():\n",
    "        sentence = row[\"preprocess_sentence\"]\n",
    "        encoded_sentence = vocabulary(tokenizer(sentence))\n",
    "        label = row[\"label\"]\n",
    "        yield encoded_sentence, label\n",
    "\n",
    "\n",
    "train_dataset = prepare_dataset(train_df_vi)\n",
    "train_dataset = to_map_style_dataset(train_dataset)\n",
    "valid_dataset = prepare_dataset(valid_df)\n",
    "valid_dataset = to_map_style_dataset(valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_batch(batch):\n",
    "    # create inputs, offsets, labels for batch\n",
    "    encoded_sentences, labels = [], []\n",
    "    for encoded_sentence, label in batch:\n",
    "        labels.append(label)\n",
    "        encoded_sentence = torch.tensor(encoded_sentence)\n",
    "        encoded_sentences.append(encoded_sentence)\n",
    "    labels = torch.tensor(labels, dtype=torch.int64)\n",
    "    encoded_sentences = nn.utils.rnn.pad_sequence(\n",
    "        encoded_sentences, padding_value=vocabulary[\"<pad>\"]\n",
    "    )\n",
    "    return encoded_sentences, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_batch\n",
    ")\n",
    "valid_dataloader = DataLoader(\n",
    "    valid_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_batch\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextCNN(nn.Module):\n",
    "    def __init__(\n",
    "        self, vocab_size, embedding_dim, kernel_sizes, num_filters, num_classes\n",
    "    ):\n",
    "        super(TextCNN, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.kernel_sizes = kernel_sizes\n",
    "        self.num_filters = num_filters\n",
    "        self.num_classes = num_classes\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.conv = nn.ModuleList(\n",
    "            [\n",
    "                nn.Conv1d(\n",
    "                    in_channels=embedding_dim,\n",
    "                    out_channels=num_filters,\n",
    "                    kernel_size=k,\n",
    "                    stride=1,\n",
    "                )\n",
    "                for k in kernel_sizes\n",
    "            ]\n",
    "        )\n",
    "        self.fc = nn.Linear(len(kernel_sizes) * num_filters, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, sequence_length = x.shape\n",
    "        x = self.embedding(x.T).transpose(1, 2)\n",
    "        x = [F.relu(conv(x)) for conv in self.conv]\n",
    "        x = [F.max_pool1d(c, c.size(-1)).squeeze(dim=-1) for c in x]\n",
    "        x = torch.cat(x, dim=1)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    model, optimizer, criterion, train_dataloader, device, epoch=0, log_interval=50\n",
    "):\n",
    "    model.train()\n",
    "    total_acc, total_count = 0, 0\n",
    "    losses = []\n",
    "    start_time = time.time()\n",
    "    for idx, (inputs, labels) in enumerate(train_dataloader):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        # zero grad\n",
    "        optimizer.zero_grad()\n",
    "        # predictions\n",
    "        predictions = model(inputs)\n",
    "        # compute loss\n",
    "        loss = criterion(predictions, labels)\n",
    "        losses.append(loss.item())\n",
    "        # backward\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_acc += (predictions.argmax(1) == labels).sum().item()\n",
    "        total_count += labels.size(0)\n",
    "        if idx % log_interval == 0 and idx > 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            print(\n",
    "                \"| epoch {:3d} | {:5d}/{:5d} batches \"\n",
    "                \"| accuracy {:8.3f}\".format(\n",
    "                    epoch, idx, len(train_dataloader), total_acc / total_count\n",
    "                )\n",
    "            )\n",
    "            total_acc, total_count = 0, 0\n",
    "            start_time = time.time()\n",
    "    epoch_acc = total_acc / total_count\n",
    "    epoch_loss = sum(losses) / len(losses)\n",
    "    return epoch_acc, epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "def evaluate(model, criterion, valid_dataloader, device):\n",
    "    model.eval()\n",
    "    total_acc, total_count = 0, 0\n",
    "    losses = []\n",
    "    with torch.no_grad():\n",
    "        for idx, (inputs, labels) in enumerate(valid_dataloader):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            # predictions\n",
    "            predictions = model(inputs)\n",
    "            # compute loss\n",
    "            loss = criterion(predictions, labels)\n",
    "            losses.append(loss.item())\n",
    "            total_acc += (predictions.argmax(1) == labels).sum().item()\n",
    "            total_count += labels.size(0)\n",
    "        epoch_acc = total_acc / total_count\n",
    "        epoch_loss = sum(losses) / len(losses)\n",
    "    return epoch_acc, epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_class = 2\n",
    "vocab_size = len(vocabulary)\n",
    "embedding_dim = 100\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = TextCNN(\n",
    "    vocab_size=vocab_size,\n",
    "    embedding_dim=embedding_dim,\n",
    "    kernel_sizes=[3, 4, 5],\n",
    "    num_filters=100,\n",
    "    num_classes=2,\n",
    ")\n",
    "model.to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "save_model = \"./model\"\n",
    "train_accs, eval_accs = [], []\n",
    "train_losses, eval_losses = [], []\n",
    "best_loss_eval = 100\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    # Training\n",
    "    train_acc, train_loss = train(\n",
    "        model, optimizer, criterion, train_dataloader, device, epoch\n",
    "    )\n",
    "    train_accs.append(train_acc)\n",
    "    train_losses.append(train_loss)\n",
    "    # Evaluation\n",
    "    eval_acc, eval_loss = evaluate(model, criterion, valid_dataloader, device)\n",
    "    eval_accs.append(eval_acc)\n",
    "    eval_losses.append(eval_loss)\n",
    "    # Save best model\n",
    "    if eval_loss < best_loss_eval:\n",
    "        torch.save(model.state_dict(), save_model + \"/text_cnn_model.pt\")\n",
    "        best_loss_eval = eval_loss\n",
    "    # Print loss, acc end epoch\n",
    "    print(\"-\" * 59)\n",
    "    print(\n",
    "        \"| End of epoch {:3d} | Time: {:5.2f}s | Train Accuracy {:8.3f} | Train Loss {:8.3f} \"\n",
    "        \"| Valid Accuracy {:8.3f} | Valid Loss {:8.3f} \".format(\n",
    "            epoch,\n",
    "            time.time() - epoch_start_time,\n",
    "            train_acc,\n",
    "            train_loss,\n",
    "            eval_acc,\n",
    "            eval_loss,\n",
    "        )\n",
    "    )\n",
    "    print(\"-\" * 59)\n",
    "\n",
    "# Load best model\n",
    "model.load_state_dict(torch.load(save_model + \"/text_cnn_model.pt\"))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = prepare_dataset(test_df)\n",
    "test_dataset = to_map_style_dataset(test_dataset)\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_batch\n",
    ")\n",
    "test_acc, test_loss = evaluate(model, criterion, test_dataloader, device)\n",
    "print(\"Test Accuracy: \", test_acc)\n",
    "print(\"Test Loss: \", test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aio2024-homework",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
