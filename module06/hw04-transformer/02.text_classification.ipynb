{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import re\n",
    "import string\n",
    "import time\n",
    "from torchtext.data import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.data.functional import to_map_style_dataset\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "\n",
    "from transformer import TransformerEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/aio2024-homework/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading readme: 100%|██████████| 570/570 [00:00<00:00, 1.97kB/s]\n",
      "Downloading data: 100%|██████████| 18.8M/18.8M [00:02<00:00, 9.37MB/s]\n",
      "Downloading data: 100%|██████████| 6.35M/6.35M [00:01<00:00, 5.69MB/s]\n",
      "Downloading data: 100%|██████████| 6.35M/6.35M [00:01<00:00, 5.54MB/s]\n",
      "Generating train split: 100%|██████████| 30000/30000 [00:00<00:00, 700611.47 examples/s]\n",
      "Generating valid split: 100%|██████████| 10000/10000 [00:00<00:00, 1010091.51 examples/s]\n",
      "Generating test split: 100%|██████████| 10000/10000 [00:00<00:00, 1063815.15 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence', 'label', 'preprocessed_sentence'],\n",
       "        num_rows: 30000\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['sentence', 'label', 'preprocessed_sentence'],\n",
       "        num_rows: 10000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence', 'label', 'preprocessed_sentence'],\n",
       "        num_rows: 10000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = load_dataset(\"thainq107/ntc-scv\")\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # remove URLs https://www.\n",
    "    url_pattern = re.compile(r\"https?://\\s+\\wwww\\.\\s+\")\n",
    "    text = url_pattern.sub(r\" \", text)\n",
    "    # remove HTML Tags: <>\n",
    "    html_pattern = re.compile(r\"<[^<>]+>\")\n",
    "    text = html_pattern.sub(\" \", text)\n",
    "    # remove puncs and digits\n",
    "    replace_chars = list(string.punctuation + string.digits)\n",
    "    for char in replace_chars:\n",
    "        text = text.replace(char, \"\")\n",
    "    # remove emoji\n",
    "    emoji_pattern = re.compile(\n",
    "        \"[\"\n",
    "        \"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        \"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        \"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        \"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        \"\\U0001F1F2-\\U0001F1F4\"  # Macau flag\n",
    "        \"\\U0001F1E6-\\U0001F1FF\"  # flags\n",
    "        \"\\U0001F600-\\U0001F64F\"\n",
    "        \"\\U00002702-\\U000027B0\"\n",
    "        \"\\U000024C2-\\U0001F251\"\n",
    "        \"\\U0001f926-\\U0001f937\"\n",
    "        \"\\U0001F1F2\"\n",
    "        \"\\U0001F1F4\"\n",
    "        \"\\U0001F620\"\n",
    "        \"\\u200d\"\n",
    "        \"\\u2640-\\u2642\"\n",
    "        \"]+\",\n",
    "        flags=re.UNICODE,\n",
    "    )\n",
    "    text = emoji_pattern.sub(r\"\", text)\n",
    "    # normalize whitespace\n",
    "    text = \" \".join(text.split())\n",
    "    # lowercasing\n",
    "    text = text.lower()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yield_tokens(sentences, tokenizer):\n",
    "    for sentence in sentences:\n",
    "        yield tokenizer(sentence)\n",
    "\n",
    "\n",
    "# word-based tokenizer\n",
    "tokenizer = get_tokenizer(\"basic_english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<pad>', '<unk>', 'ăn', 'mình', 'có', 'là', 'không', 'quán', 'thì', 'và']\n"
     ]
    }
   ],
   "source": [
    "# build vocabulary\n",
    "vocab_size = 10000\n",
    "vocabulary = build_vocab_from_iterator(\n",
    "    yield_tokens(ds[\"train\"][\"preprocessed_sentence\"], tokenizer),\n",
    "    max_tokens=vocab_size,\n",
    "    specials=[\"<pad>\", \"<unk>\"],\n",
    ")\n",
    "vocabulary.set_default_index(vocabulary[\"<unk>\"])\n",
    "print(vocabulary.get_itos()[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([351, 111, 112, 529, 124, 228, 196, 53, 159, 43, 103, 256, 46, 2, 11, 31, 52, 723, 32, 491, 991, 533, 32, 491, 220, 1415, 9, 731, 897, 185, 130, 836, 57, 88, 4, 14, 3183, 251, 59], 1)\n",
      "([5, 7, 322, 221, 256, 21, 28, 116, 84, 689, 584, 3, 950, 102, 221, 254, 9, 221, 25, 581, 258, 53, 13, 11, 21, 59, 59, 255, 72, 65, 324, 1331, 737, 768, 49, 371, 340, 13, 86, 244, 30, 3, 9, 317, 58, 96, 5, 59, 300, 270, 650, 4214, 297, 1201, 81, 756, 701, 441, 180, 17, 2, 88, 5, 1576, 215, 33, 401, 359, 677, 439, 1555, 3, 6, 62, 181, 167, 72, 221, 5, 300, 11, 9, 300, 47, 66, 17, 192, 29, 81, 1071, 43, 246, 8, 52, 7, 13, 110, 56, 394, 167, 2, 359, 17, 38, 728, 42, 162, 235, 90, 1690, 116, 235, 69, 6, 28, 2, 200, 3, 90, 141, 87, 68, 2, 33, 312, 301, 17, 189, 2, 273, 7, 2545, 484, 269, 795, 4, 1331, 109, 1976, 10, 28, 490, 9, 703, 210, 701, 104, 68, 104, 457, 95, 1331, 221, 57], 1)\n",
      "([584, 183, 3, 131, 984, 87, 598, 576, 9, 357, 61, 1008, 36, 365, 192, 1811, 23, 81, 41, 108, 113, 16, 176, 133, 290, 20, 84, 87, 21, 598, 576, 61, 1008, 42, 268, 480, 376, 466, 445, 94, 263, 478, 1107, 586, 6, 60, 751, 687, 54, 61, 1008, 7278, 18, 13, 275, 392, 144, 321, 191, 341, 614, 1769, 361, 2082, 23, 207, 290, 5, 81, 795, 107, 94, 100, 345, 3, 1571, 341, 64, 33, 210, 183, 276, 3330, 341, 1312, 19, 20, 17, 365, 192, 1811, 52, 172, 114, 355, 31, 276, 346, 17, 19, 81, 41, 19, 20, 29, 1138, 196], 1)\n",
      "30000\n",
      "10000\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "# convert torchtext dataset\n",
    "def prepare_dataset(df):\n",
    "    for row in df:\n",
    "        sentence = row[\"preprocessed_sentence\"]\n",
    "        encoded_sentence = vocabulary(tokenizer(sentence))\n",
    "        label = row[\"label\"]\n",
    "        yield encoded_sentence, label\n",
    "\n",
    "\n",
    "train_dataset = prepare_dataset(ds[\"train\"])\n",
    "train_dataset = to_map_style_dataset(train_dataset)\n",
    "\n",
    "valid_dataset = prepare_dataset(ds[\"valid\"])\n",
    "valid_dataset = to_map_style_dataset(valid_dataset)\n",
    "\n",
    "test_dataset = prepare_dataset(ds[\"test\"])\n",
    "test_dataset = to_map_style_dataset(test_dataset)\n",
    "\n",
    "print(train_dataset[0])\n",
    "print(valid_dataset[0])\n",
    "print(test_dataset[0])\n",
    "\n",
    "# print shape of datasets\n",
    "print(len(train_dataset))\n",
    "print(len(valid_dataset))\n",
    "print(len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 100\n",
    "\n",
    "\n",
    "def collate_batch(batch):\n",
    "    sentences, labels = list(zip(*batch))\n",
    "    encoded_sentences = [\n",
    "        (\n",
    "            sentence + ([0] * (seq_length - len(sentence)))\n",
    "            if len(sentence) < seq_length\n",
    "            else sentence[:seq_length]\n",
    "        )\n",
    "        for sentence in sentences\n",
    "    ]\n",
    "    encoded_sentences = torch.tensor(encoded_sentences)\n",
    "    labels = torch.tensor(labels)\n",
    "    return encoded_sentences, labels\n",
    "\n",
    "\n",
    "batch_size = 128\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_batch\n",
    ")\n",
    "\n",
    "valid_dataloader = DataLoader(\n",
    "    valid_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_batch\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_batch\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderCls(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size,\n",
    "        max_length,\n",
    "        num_layers,\n",
    "        embed_dim,\n",
    "        num_heads,\n",
    "        ff_dim,\n",
    "        dropout=0.1,\n",
    "        device=\"cpu\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.encoder = TransformerEncoder(\n",
    "            vocab_size,\n",
    "            embed_dim,\n",
    "            max_length,\n",
    "            num_layers,\n",
    "            num_heads,\n",
    "            ff_dim,\n",
    "            dropout,\n",
    "            device,\n",
    "        )\n",
    "        self.pooling = nn.AvgPool1d(kernel_size=max_length)\n",
    "        self.fc1 = nn.Linear(in_features=embed_dim, out_features=20)\n",
    "        self.fc2 = nn.Linear(in_features=20, out_features=2)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.encoder(x)\n",
    "        output = self.pooling(output.permute(0, 2, 1)).squeeze()\n",
    "        output = self.dropout(output)\n",
    "        output = self.fc1(output)\n",
    "        output = self.relu(output)\n",
    "        output = self.dropout(output)\n",
    "        output = self.fc2(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(\n",
    "    model, optimizer, criterion, train_dataloader, device, epoch=0, log_interval=50\n",
    "):\n",
    "    model.train()\n",
    "    total_acc, total_count = 0, 0\n",
    "    losses = []\n",
    "    start_time = time.time()\n",
    "    for idx, (inputs, labels) in enumerate(train_dataloader):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(inputs)\n",
    "        # compute loss\n",
    "        loss = criterion(predictions, labels)\n",
    "        losses.append(loss.item())\n",
    "        # backward\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_acc += (predictions.argmax(1) == labels).sum().item()\n",
    "        total_count += labels.size(0)\n",
    "        if idx % log_interval == 0 and idx > 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            print(\n",
    "                \"| epoch {:3d} | {:5d}/{:5d} batches \"\n",
    "                \"| accuracy {:8.3f}\".format(\n",
    "                    epoch, idx, len(train_dataloader), total_acc / total_count\n",
    "                )\n",
    "            )\n",
    "            total_acc, total_count = 0, 0\n",
    "            start_time = time.time()\n",
    "    epoch_acc = total_acc / total_count\n",
    "    epoch_loss = sum(losses) / len(losses)\n",
    "    return epoch_acc, epoch_loss\n",
    "\n",
    "\n",
    "def evaluate_epoch(model, criterion, valid_dataloader, device):\n",
    "    model.eval()\n",
    "    total_acc, total_count = 0, 0\n",
    "    losses = []\n",
    "    with torch.no_grad():\n",
    "        for idx, (inputs, labels) in enumerate(valid_dataloader):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            predictions = model(inputs)\n",
    "            loss = criterion(predictions, labels)\n",
    "            losses.append(loss.item())\n",
    "            total_acc += (predictions.argmax(1) == labels).sum().item()\n",
    "            total_count += labels.size(0)\n",
    "\n",
    "    epoch_acc = total_acc / total_count\n",
    "    epoch_loss = sum(losses) / len(losses)\n",
    "    return epoch_acc, epoch_loss\n",
    "\n",
    "\n",
    "def train(\n",
    "    model,\n",
    "    model_name,\n",
    "    save_model,\n",
    "    optimizer,\n",
    "    criterion,\n",
    "    train_dataloader,\n",
    "    valid_dataloader,\n",
    "    num_epochs,\n",
    "    device,\n",
    "):\n",
    "    train_accs, train_losses = [], []\n",
    "    eval_accs, eval_losses = [], []\n",
    "    best_loss_eval = 100\n",
    "    times = []\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        epoch_start_time = time.time()\n",
    "        # Training\n",
    "        train_acc, train_loss = train_epoch(\n",
    "            model, optimizer, criterion, train_dataloader, device, epoch\n",
    "        )\n",
    "        train_accs.append(train_acc)\n",
    "        train_losses.append(train_loss)\n",
    "        # Evaluation\n",
    "        eval_acc, eval_loss = evaluate_epoch(model, criterion, valid_dataloader, device)\n",
    "        eval_accs.append(eval_acc)\n",
    "        eval_losses.append(eval_loss)\n",
    "        # Save best model\n",
    "        if eval_loss < best_loss_eval:\n",
    "            torch.save(model.state_dict(), save_model + f\"/{model_name}.pt\")\n",
    "            best_loss_eval = eval_loss\n",
    "        times.append(time.time() - epoch_start_time)\n",
    "        # Print loss, acc end epoch\n",
    "        print(\"-\" * 59)\n",
    "        print(\n",
    "            \"| End of epoch {:3d} | Time: {:5.2f}s | Train Accuracy {:8.3f} | Train Loss {:8.3f} \"\n",
    "            \"| Valid Accuracy {:8.3f} | Valid Loss {:8.3f} \".format(\n",
    "                epoch,\n",
    "                time.time() - epoch_start_time,\n",
    "                train_acc,\n",
    "                train_loss,\n",
    "                eval_acc,\n",
    "                eval_loss,\n",
    "            )\n",
    "        )\n",
    "        print(\"-\" * 59)\n",
    "\n",
    "    # Load best model\n",
    "    model.load_state_dict(torch.load(save_model + f\"/{model_name}.pt\"))\n",
    "    model.eval()\n",
    "    metrics = {\n",
    "        \"train_accuracy\": train_accs,\n",
    "        \"train_loss\": train_losses,\n",
    "        \"valid_accuracy\": eval_accs,\n",
    "        \"valid_loss\": eval_losses,\n",
    "        \"time\": times,\n",
    "    }\n",
    "    return model, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_result(num_epochs, train_accs, eval_accs, train_losses, eval_losses):\n",
    "    epochs = list(range(num_epochs))\n",
    "    fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(12, 6))\n",
    "    axs[0].plot(epochs, train_accs, label=\"Training\")\n",
    "    axs[0].plot(epochs, eval_accs, label=\"Evaluation\")\n",
    "    axs[1].plot(epochs, train_losses, label=\"Training\")\n",
    "    axs[1].plot(epochs, eval_losses, label=\"Evaluation\")\n",
    "    axs[0].set_xlabel(\"Epochs\")\n",
    "    axs[1].set_xlabel(\"Epochs\")\n",
    "    axs[0].set_ylabel(\"Accuracy\")\n",
    "    axs[1].set_ylabel(\"Loss\")\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 10000\n",
    "max_length = 100\n",
    "embed_dim = 200\n",
    "num_layers = 2\n",
    "num_heads = 4\n",
    "ff_dim = 128\n",
    "dropout = 0.1\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = TransformerEncoderCls(\n",
    "    vocab_size, max_length, num_layers, embed_dim, num_heads, ff_dim, dropout, device\n",
    ")\n",
    "model.to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.00005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 50\n",
    "save_model = \"./model\"\n",
    "model_name = \"model\"\n",
    "model, metrics = train(\n",
    "    model,\n",
    "    model_name,\n",
    "    save_model,\n",
    "    optimizer,\n",
    "    criterion,\n",
    "    train_dataloader,\n",
    "    valid_dataloader,\n",
    "    num_epochs,\n",
    "    device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_result(\n",
    "    num_epochs,\n",
    "    metrics[\"train_accuracy\"],\n",
    "    metrics[\"valid_accuracy\"],\n",
    "    metrics[\"train_loss\"],\n",
    "    metrics[\"valid_loss\"],\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aio2024-homework",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
