{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a951c038",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q unsloth==2025.4.7 datasets==3.5.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c728e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b8b54aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ü¶• Unsloth Zoo will now patch everything to make training faster!\n",
      "==((====))==  Unsloth 2025.4.7: Fast Llama patching. Transformers: 4.51.3.\n",
      "   \\\\   /|    NVIDIA RTX A5000. Num GPUs = 1. Max memory: 23.673 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.7.0+cu126. CUDA: 8.6. CUDA Toolkit: 12.6. Triton: 3.3.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.30. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.4.7 patched 16 layers with 16 QKV layers, 16 O layers and 16 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from unsloth import FastLanguageModel\n",
    "\n",
    "\n",
    "MAX_SEQ_LENGTH = 2048\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\",\n",
    "    max_seq_length=MAX_SEQ_LENGTH,\n",
    "    load_in_4bit=True,\n",
    "    dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=16,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0,\n",
    "    target_modules=[\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"up_proj\",\n",
    "        \"down_proj\", \"o_proj\", \"gate_proj\"],\n",
    "    use_rslora=True,\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    "    random_state = 42,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c63b94f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 11,272,192 || all params: 1,247,086,592 || trainable%: 0.9039\n"
     ]
    }
   ],
   "source": [
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92168921",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset_name = \"5CD-AI/Vietnamese-Multi-turn-Chat-Alpaca\"\n",
    "SYS_INSTRUCT = \"B·∫°n l√† m·ªôt tr·ª£ l√Ω AI th√¢n thi·ªán, h√£y tr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát.\"\n",
    "raw_dataset = load_dataset(dataset_name, split=\"train\")\n",
    "\n",
    "def convert_to_chat_format(conversations):\n",
    "    messages = [{\"role\": \"system\", \"content\": SYS_INSTRUCT}]\n",
    "    for msg in conversations:\n",
    "        role = \"user\" if msg[\"from\"] == \"human\" else \"assistant\"\n",
    "        messages.append({\"role\": role, \"content\": msg[\"value\"]})\n",
    "    return messages\n",
    "\n",
    "def format_prompt(example):\n",
    "    messages = convert_to_chat_format(example[\"conversations\"])\n",
    "    return {\n",
    "        \"text\": tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=False\n",
    "        )\n",
    "    }\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=MAX_SEQ_LENGTH,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "dataset = raw_dataset.map(format_prompt, remove_columns=raw_dataset.column_names)\n",
    "dataset = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98776e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 12,697 | Num Epochs = 5 | Total steps = 400\n",
      "O^O/ \\_/ \\    Batch size per device = 64 | Gradient accumulation steps = 2\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (64 x 2 x 1) = 128\n",
      " \"-____-\"     Trainable parameters = 11,272,192/1,000,000,000 (1.13% trained)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='400' max='400' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [400/400 3:34:01, Epoch 4/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.527100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.349700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.299000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.277400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=400, training_loss=1.3632887840270995, metrics={'train_runtime': 12874.6447, 'train_samples_per_second': 3.977, 'train_steps_per_second': 0.031, 'total_flos': 6.14358673287807e+17, 'train_loss': 1.3632887840270995})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "from trl import SFTTrainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size=32,\n",
    "    gradient_accumulation_steps=2,\n",
    "    learning_rate=1e-4,\n",
    "    save_total_limit=4,\n",
    "    logging_steps=20,\n",
    "    output_dir=\"./checkpoint/llama3-1b-multi-conversation\",\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.05,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=50,\n",
    "    report_to=\"none\",\n",
    "    remove_unused_columns=True,\n",
    "    max_steps=400,\n",
    "    bf16=True,\n",
    ")\n",
    "\n",
    "trainer=SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf27897",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ü¶• Unsloth Zoo will now patch everything to make training faster!\n",
      "==((====))==  Unsloth 2025.4.7: Fast Llama patching. Transformers: 4.51.3.\n",
      "   \\\\   /|    NVIDIA RTX A5000. Num GPUs = 3. Max memory: 23.673 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.7.0+cu126. CUDA: 8.6. CUDA Toolkit: 12.6. Triton: 3.3.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.30. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.4.7 patched 16 layers with 16 QKV layers, 16 O layers and 16 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "# load model from ckpt dir then save to huggingface hub\n",
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    \"./checkpoint/llama3-1b-multi-conversation/checkpoint-400\",\n",
    "    max_seq_length=2048,\n",
    "    load_in_4bit=True,\n",
    "    dtype=torch.bfloat16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba45f418",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "peft.peft_model.PeftModelForCausalLM"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "432326af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: You're not saving a tokenizer as well?\n",
      "You can do it separately via `tokenizer.push_to_hub(...)`\n",
      "Unsloth: You are pushing to hub, but you passed your HF username = thuanan.\n",
      "We shall truncate thuanan/Llama-3.2-1B-Instruct-Chat-sft to Llama-3.2-1B-Instruct-Chat-sft\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging 4bit and LoRA weights to 16bit...\n",
      "Unsloth: Will use up to 83.71 out of 125.4 RAM for saving.\n",
      "Unsloth: Saving model... This might take 5 minutes ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:00<00:00, 60.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "058c8406192142758ed5e17e9e41cbc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43a161fe0dca45a88576579ebce1c7cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.47G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n",
      "Saved merged model to https://huggingface.co/thuanan/Llama-3.2-1B-Instruct-Chat-sft\n"
     ]
    }
   ],
   "source": [
    "model.push_to_hub_merged(\n",
    "    \"thuanan/Llama-3.2-1B-Instruct-Chat-sft\",\n",
    "    commit_message=\"Merge weights to push to hub\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35e7a403",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.tokenization_utils_fast.PreTrainedTokenizerFast"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "047c5382",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a347a3d64e17420c918784219c51a2cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "086651b0cccd4c39b845bdcb1ace0898",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer.push_to_hub(\n",
    "    \"thuanan/Llama-3.2-1B-Instruct-Chat-sft\",\n",
    "    commit_message=\"Push tokenizer to hub\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "50827f80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant reply: Ch·∫Øc ch·∫Øn! D∆∞·ªõi ƒë√¢y l√† m·ªôt v√†i m·∫πo v·ªÅ c√°ch c√°c nh√† t·ªï ch·ª©c v√† c·∫ßu th·ªß team c√≥ th·ªÉ √°p d·ª•ng cho tr√≤ ch∆°i s·∫Øp t·ªõi: - ƒê·∫£m b·∫£o r·∫±ng t·∫•t c·∫£ nh·ªØng ng∆∞·ªùi tham gia ƒë·ªÅu ƒë∆∞·ª£c ƒë√†o t·∫°o ƒë·∫ßy ƒë·ªß tr∆∞·ªõc khi b·∫Øt ƒë·∫ßu cu·ªôc ƒëua ho·∫∑c gi·∫£i tr√≠ m·ªõi nh·∫•t. ƒêi·ªÅu quan tr·ªçng c·∫ßn l∆∞u t√¢m l√† vi·ªác chu·∫©n b·ªã k·ªπ l∆∞·ª°ng s·∫Ω gi√∫p gi·∫£m thi·ªÉu r·ªßi ro x·∫£y ra s·ª± c·ªë tr√™n s√¢n kh·∫•u c≈©ng nh∆∞ ƒë·∫£m b·∫£o m·ªçi th·ª© di·ªÖn ra su√¥n s·∫ª v·ªõi t∆∞ duy s√°ng su·ªët t·ªët. - T√¨m ki·∫øm ph·∫£n h·ªìi t·ª´ ƒë·ªìng nghi·ªáp v√† gi√°m s√°t t·ª´ng b∆∞·ªõc h√†nh ƒë·ªông c·ªßa b·∫°n nh·∫±m x√°c ƒë·ªãnh\n"
     ]
    }
   ],
   "source": [
    "from transformers import GenerationConfig\n",
    "\n",
    "generation_config = GenerationConfig(\n",
    "    max_new_tokens=128,\n",
    "    temperature=1.0,\n",
    "    do_sample=False,\n",
    "    num_return_sequences=1,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    repetition_penalty=1.3\n",
    ")\n",
    "\n",
    "\n",
    "prompt = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"B·∫°n l√† m·ªôt tr·ª£ l√Ω AI th√¢n thi·ªán, h√£y tr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"\"\"H√£y ch·ªânh s·ª≠a c√¢u n√†y ƒë·ªÉ ng·∫Øn g·ªçn h∆°n m√† kh√¥ng m·∫•t ƒëi √Ω nghƒ©a: \"Tr·∫≠n ƒë·∫•u l√† m·ªôt th·∫•t b·∫°i n·∫∑ng n·ªÅ m·∫∑c d√π th·ª±c t·∫ø l√† c·∫£ ƒë·ªôi ƒë√£ t·∫≠p luy·ªán trong nhi·ªÅu tu·∫ßn.\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"\"\"Nhi·ªÅu tu·∫ßn hu·∫•n luy·ªán c·ªßa ƒë·ªôi ƒë√£ d·∫´n ƒë·∫øn m·ªôt th·∫•t b·∫°i n·∫∑ng n·ªÅ.\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"\"\"B·∫°n c√≥ th·ªÉ ƒë·ªÅ xu·∫•t m·ªôt s·ªë chi·∫øn l∆∞·ª£c m√† nh√≥m c√≥ th·ªÉ s·ª≠ d·ª•ng ƒë·ªÉ c·∫£i thi·ªán hi·ªáu su·∫•t c·ªßa h·ªç trong tr·∫≠n ƒë·∫•u ti·∫øp theo kh√¥ng?\"\"\"\n",
    "    }\n",
    "]\n",
    "\n",
    "chat_text = tokenizer.apply_chat_template(\n",
    "    prompt,\n",
    "    add_generation_prompt=True,\n",
    "    tokenize=False\n",
    ")\n",
    "\n",
    "inputs = tokenizer(\n",
    "    chat_text,\n",
    "    return_tensors=\"pt\"\n",
    ").to(\"cuda:0\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        input_ids=inputs[\"input_ids\"],\n",
    "        attention_mask=inputs[\"attention_mask\"],\n",
    "        generation_config=generation_config,\n",
    "    )\n",
    "output_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "if \"assistant\\n\" in output_text:\n",
    "    answer = output_text.split(\"assistant\\n\")[-1].strip()\n",
    "else:\n",
    "    answer = output_text.strip()\n",
    "\n",
    "print(\"Assistant reply:\", answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-rlhf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
