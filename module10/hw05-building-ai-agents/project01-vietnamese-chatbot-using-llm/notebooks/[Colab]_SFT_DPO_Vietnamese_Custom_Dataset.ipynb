{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7e77c879",
      "metadata": {
        "id": "7e77c879"
      },
      "outputs": [],
      "source": [
        "!pip install -U trl transformers datasets bitsandbytes peft wandb accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a6c61671",
      "metadata": {
        "id": "a6c61671"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "import torch\n",
        "\n",
        "import transformers\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, BitsAndBytesConfig\n",
        "from datasets import load_dataset\n",
        "from peft import LoraConfig, PeftModel, get_peft_model, prepare_model_for_kbit_training\n",
        "from trl import DPOTrainer, DPOConfig, SFTConfig, SFTTrainer\n",
        "import bitsandbytes as bnb\n",
        "\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f81a8ffe",
      "metadata": {
        "id": "f81a8ffe"
      },
      "outputs": [],
      "source": [
        "model_name = f\"meta-llama/Llama-3.2-1B-Instruct\"\n",
        "cache_dir = \"./cache\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c0ef4f3d",
      "metadata": {
        "id": "c0ef4f3d",
        "outputId": "6d440bf1-00d5-4df3-8ca0-4852d39149f1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "LlamaForCausalLM(\n",
              "  (model): LlamaModel(\n",
              "    (embed_tokens): Embedding(128256, 2048)\n",
              "    (layers): ModuleList(\n",
              "      (0-15): 16 x LlamaDecoderLayer(\n",
              "        (self_attn): LlamaAttention(\n",
              "          (q_proj): lora.Linear4bit(\n",
              "            (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
              "            (lora_dropout): ModuleDict(\n",
              "              (default): Dropout(p=0.05, inplace=False)\n",
              "            )\n",
              "            (lora_A): ModuleDict(\n",
              "              (default): Linear(in_features=2048, out_features=16, bias=False)\n",
              "            )\n",
              "            (lora_B): ModuleDict(\n",
              "              (default): Linear(in_features=16, out_features=2048, bias=False)\n",
              "            )\n",
              "            (lora_embedding_A): ParameterDict()\n",
              "            (lora_embedding_B): ParameterDict()\n",
              "            (lora_magnitude_vector): ModuleDict()\n",
              "          )\n",
              "          (k_proj): lora.Linear4bit(\n",
              "            (base_layer): Linear4bit(in_features=2048, out_features=512, bias=False)\n",
              "            (lora_dropout): ModuleDict(\n",
              "              (default): Dropout(p=0.05, inplace=False)\n",
              "            )\n",
              "            (lora_A): ModuleDict(\n",
              "              (default): Linear(in_features=2048, out_features=16, bias=False)\n",
              "            )\n",
              "            (lora_B): ModuleDict(\n",
              "              (default): Linear(in_features=16, out_features=512, bias=False)\n",
              "            )\n",
              "            (lora_embedding_A): ParameterDict()\n",
              "            (lora_embedding_B): ParameterDict()\n",
              "            (lora_magnitude_vector): ModuleDict()\n",
              "          )\n",
              "          (v_proj): lora.Linear4bit(\n",
              "            (base_layer): Linear4bit(in_features=2048, out_features=512, bias=False)\n",
              "            (lora_dropout): ModuleDict(\n",
              "              (default): Dropout(p=0.05, inplace=False)\n",
              "            )\n",
              "            (lora_A): ModuleDict(\n",
              "              (default): Linear(in_features=2048, out_features=16, bias=False)\n",
              "            )\n",
              "            (lora_B): ModuleDict(\n",
              "              (default): Linear(in_features=16, out_features=512, bias=False)\n",
              "            )\n",
              "            (lora_embedding_A): ParameterDict()\n",
              "            (lora_embedding_B): ParameterDict()\n",
              "            (lora_magnitude_vector): ModuleDict()\n",
              "          )\n",
              "          (o_proj): lora.Linear4bit(\n",
              "            (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
              "            (lora_dropout): ModuleDict(\n",
              "              (default): Dropout(p=0.05, inplace=False)\n",
              "            )\n",
              "            (lora_A): ModuleDict(\n",
              "              (default): Linear(in_features=2048, out_features=16, bias=False)\n",
              "            )\n",
              "            (lora_B): ModuleDict(\n",
              "              (default): Linear(in_features=16, out_features=2048, bias=False)\n",
              "            )\n",
              "            (lora_embedding_A): ParameterDict()\n",
              "            (lora_embedding_B): ParameterDict()\n",
              "            (lora_magnitude_vector): ModuleDict()\n",
              "          )\n",
              "        )\n",
              "        (mlp): LlamaMLP(\n",
              "          (gate_proj): lora.Linear4bit(\n",
              "            (base_layer): Linear4bit(in_features=2048, out_features=8192, bias=False)\n",
              "            (lora_dropout): ModuleDict(\n",
              "              (default): Dropout(p=0.05, inplace=False)\n",
              "            )\n",
              "            (lora_A): ModuleDict(\n",
              "              (default): Linear(in_features=2048, out_features=16, bias=False)\n",
              "            )\n",
              "            (lora_B): ModuleDict(\n",
              "              (default): Linear(in_features=16, out_features=8192, bias=False)\n",
              "            )\n",
              "            (lora_embedding_A): ParameterDict()\n",
              "            (lora_embedding_B): ParameterDict()\n",
              "            (lora_magnitude_vector): ModuleDict()\n",
              "          )\n",
              "          (up_proj): lora.Linear4bit(\n",
              "            (base_layer): Linear4bit(in_features=2048, out_features=8192, bias=False)\n",
              "            (lora_dropout): ModuleDict(\n",
              "              (default): Dropout(p=0.05, inplace=False)\n",
              "            )\n",
              "            (lora_A): ModuleDict(\n",
              "              (default): Linear(in_features=2048, out_features=16, bias=False)\n",
              "            )\n",
              "            (lora_B): ModuleDict(\n",
              "              (default): Linear(in_features=16, out_features=8192, bias=False)\n",
              "            )\n",
              "            (lora_embedding_A): ParameterDict()\n",
              "            (lora_embedding_B): ParameterDict()\n",
              "            (lora_magnitude_vector): ModuleDict()\n",
              "          )\n",
              "          (down_proj): lora.Linear4bit(\n",
              "            (base_layer): Linear4bit(in_features=8192, out_features=2048, bias=False)\n",
              "            (lora_dropout): ModuleDict(\n",
              "              (default): Dropout(p=0.05, inplace=False)\n",
              "            )\n",
              "            (lora_A): ModuleDict(\n",
              "              (default): Linear(in_features=8192, out_features=16, bias=False)\n",
              "            )\n",
              "            (lora_B): ModuleDict(\n",
              "              (default): Linear(in_features=16, out_features=2048, bias=False)\n",
              "            )\n",
              "            (lora_embedding_A): ParameterDict()\n",
              "            (lora_embedding_B): ParameterDict()\n",
              "            (lora_magnitude_vector): ModuleDict()\n",
              "          )\n",
              "          (act_fn): SiLU()\n",
              "        )\n",
              "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
              "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
              "      )\n",
              "    )\n",
              "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
              "    (rotary_emb): LlamaRotaryEmbedding()\n",
              "  )\n",
              "  (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
              ")"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Model to fine-tune\n",
        "\n",
        "# model_name = \"thainq107/Llama-3.2-1B-Instruct-sft\" f\"meta-llama/Llama-3.2-1B-Instruct\"\n",
        "# cache_dir = \"./cache\"\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    trust_remote_code=True,\n",
        "    device_map={\"\" : torch.cuda.current_device()},\n",
        "    token=\"###\",\n",
        "    cache_dir=cache_dir,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    quantization_config=bnb_config,\n",
        ")\n",
        "base_model.config.use_cache = False\n",
        "base_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec75dcd6",
      "metadata": {
        "id": "ec75dcd6"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    model_name, cache_dir=cache_dir, trust_remote_code=True\n",
        ")\n",
        "\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    tokenizer.pad_token_id = tokenizer.eos_token_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "da17e2f2",
      "metadata": {
        "id": "da17e2f2",
        "outputId": "a2ecaae6-00aa-484a-b079-970baec4a239"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(DatasetDict({\n",
              "     train: Dataset({\n",
              "         features: ['id', 'question', 'chosen', 'rejected'],\n",
              "         num_rows: 65017\n",
              "     })\n",
              "     test: Dataset({\n",
              "         features: ['id', 'question', 'chosen', 'rejected'],\n",
              "         num_rows: 2000\n",
              "     })\n",
              " }),\n",
              " {'id': 'alpaca-7294',\n",
              "  'question': 'Xác định và sửa lỗi ngữ pháp.\\n\\nTôi đã đi đến cửa hàng.',\n",
              "  'chosen': 'Không có lỗi ngữ pháp. Câu này đã chính xác.',\n",
              "  'rejected': 'Câu này không có lỗi ngữ pháp.'})"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset = load_dataset(\"thainq107/Vi-Alpaca-Preference\")\n",
        "dataset, dataset[\"train\"][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "sXOgdWIH8E1k",
      "metadata": {
        "id": "sXOgdWIH8E1k"
      },
      "outputs": [],
      "source": [
        "# QLoRA configuration\n",
        "peft_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    target_modules=[\n",
        "        \"q_proj\",\n",
        "        \"k_proj\",\n",
        "        \"v_proj\",\n",
        "        \"o_proj\",\n",
        "        \"gate_proj\",\n",
        "        \"up_proj\",\n",
        "        \"down_proj\"\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "959a1483",
      "metadata": {
        "id": "959a1483"
      },
      "source": [
        "## **SFT**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e13c22c9",
      "metadata": {
        "id": "e13c22c9"
      },
      "outputs": [],
      "source": [
        "def formatting_prompt_with_chat_template(example):\n",
        "    conversation = [\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": example[\"question\"]},\n",
        "        {\"role\": \"assistant\", \"content\": example[\"chosen\"]},\n",
        "    ]\n",
        "    prompt = tokenizer.apply_chat_template(\n",
        "        conversation, tokenize=False, add_generation_prompt=False\n",
        "    )\n",
        "    return prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b9328c8f",
      "metadata": {
        "id": "b9328c8f",
        "outputId": "ba159db5-1136-4386-a796-00a3063562f3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'meta-llama/Llama-3.2-1B-Instruct'"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cfa182d2",
      "metadata": {
        "id": "cfa182d2"
      },
      "outputs": [],
      "source": [
        "hyperparameters = {\n",
        "    \"per_device_train_batch_size\": 16,\n",
        "    \"gradient_accumulation_steps\": 8,\n",
        "    \"gradient_checkpointing\": True,\n",
        "    \"learning_rate\": 3e-5,\n",
        "    \"logging_steps\": 200,\n",
        "    \"num_train_epochs\": 2,\n",
        "    \"save_strategy\": \"no\",\n",
        "    \"overwrite_output_dir\": True,\n",
        "    \"optim\": \"paged_adamw_8bit\",\n",
        "    \"warmup_steps\": 200,\n",
        "    \"bf16\": True,\n",
        "}\n",
        "MAX_LENGTH = 512"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "72cf0c8c",
      "metadata": {
        "id": "72cf0c8c",
        "outputId": "29db93c3-5a4e-49ad-a051-d9730e6ffc80"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/thai-nq107-aisolus/vi-alpaca-preference/runs/3j40vz2u?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x738436cfe6e0>"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Use wandb\n",
        "import wandb\n",
        "wandb.init(\n",
        "    project=\"vi-alpaca-preference\",\n",
        "    name=\"llama-3.2-1b-4bit-sft\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "902f13d2",
      "metadata": {
        "id": "902f13d2",
        "outputId": "cb23a67f-b31c-4010-da44-4b1e6c2c84ec"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1016' max='1016' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1016/1016 4:17:28, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>1.953300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>1.525300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>1.409100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>1.384000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>1.380400</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=1016, training_loss=1.5280118626872385, metrics={'train_runtime': 15463.4776, 'train_samples_per_second': 8.409, 'train_steps_per_second': 0.066, 'total_flos': 3.6872604907994726e+17, 'train_loss': 1.5280118626872385})"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "SFT_OUTPUT_DIR = f\"Llama-3.2-1B-Instruct-sft\"\n",
        "\n",
        "sft_config = SFTConfig(\n",
        "    **{ **hyperparameters, \"output_dir\":\n",
        "       SFT_OUTPUT_DIR , \"max_seq_length\": MAX_LENGTH}\n",
        ")\n",
        "sft_trainer = SFTTrainer(\n",
        "    model=base_model,\n",
        "    peft_config=peft_config,\n",
        "    processing_class=tokenizer,\n",
        "    args=sft_config,\n",
        "    train_dataset=dataset['train'],\n",
        "    formatting_func=formatting_prompt_with_chat_template\n",
        ")\n",
        "\n",
        "sft_trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aaf0ed59",
      "metadata": {
        "id": "aaf0ed59"
      },
      "source": [
        "## **DPO**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "512cdd06",
      "metadata": {
        "id": "512cdd06"
      },
      "outputs": [],
      "source": [
        "# Model to fine-tune\n",
        "\n",
        "model_name = \"thainq107/Llama-3.2-1B-Instruct-sft\"\n",
        "cache_dir = \"./cache\"\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    trust_remote_code=True,\n",
        "    device_map={\"\" : torch.cuda.current_device()},\n",
        "    token=\"###\",\n",
        "    cache_dir=cache_dir,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    quantization_config=bnb_config,\n",
        ")\n",
        "base_model.config.use_cache = False\n",
        "base_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c6429f30",
      "metadata": {
        "id": "c6429f30"
      },
      "outputs": [],
      "source": [
        "def convert_to_conversational_preference_format(example):\n",
        "    return {\n",
        "        \"id\": example[\"id\"],\n",
        "        \"prompt\": [{\"role\": \"system\",\n",
        "                    \"content\": \"You are a helpful assistant.\"},\n",
        "                   {\"role\": \"user\",\n",
        "                    \"content\": example[\"question\"]}],\n",
        "        \"chosen\": [{\"role\": \"assistant\",\n",
        "                    \"content\": example[\"chosen\"]}],\n",
        "        \"rejected\": [{\"role\": \"assistant\",\n",
        "                      \"content\": example[\"rejected\"]}],\n",
        "    }\n",
        "\n",
        "dpo_dataset = dataset.map(convert_to_conversational_preference_format)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "00b0f934",
      "metadata": {
        "id": "00b0f934",
        "outputId": "2ca61a52-cd5e-46b2-dd57-5e764632c115"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'id': 'alpaca-7294',\n",
              " 'question': 'Xác định và sửa lỗi ngữ pháp.\\n\\nTôi đã đi đến cửa hàng.',\n",
              " 'chosen': [{'content': 'Không có lỗi ngữ pháp. Câu này đã chính xác.',\n",
              "   'role': 'assistant'}],\n",
              " 'rejected': [{'content': 'Câu này không có lỗi ngữ pháp.',\n",
              "   'role': 'assistant'}],\n",
              " 'prompt': [{'content': 'You are a helpful assistant.', 'role': 'system'},\n",
              "  {'content': 'Xác định và sửa lỗi ngữ pháp.\\n\\nTôi đã đi đến cửa hàng.',\n",
              "   'role': 'user'}]}"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dpo_dataset['train'][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "60e5b468",
      "metadata": {
        "id": "60e5b468"
      },
      "outputs": [],
      "source": [
        "hyperparameters = {\n",
        "    \"per_device_train_batch_size\": 8,\n",
        "    \"gradient_accumulation_steps\": 8,\n",
        "    \"gradient_checkpointing\": True,\n",
        "    \"learning_rate\": 3e-5,\n",
        "    \"logging_steps\": 200,\n",
        "    \"num_train_epochs\": 2,\n",
        "    \"save_strategy\": \"no\",\n",
        "    \"overwrite_output_dir\": True,\n",
        "    \"optim\": \"paged_adamw_8bit\",\n",
        "    \"warmup_steps\": 200,\n",
        "    \"bf16\": True,\n",
        "}\n",
        "MAX_LENGTH = 512"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "86f941ba",
      "metadata": {
        "id": "86f941ba"
      },
      "outputs": [],
      "source": [
        "# Use wandb\n",
        "import wandb\n",
        "wandb.init(\n",
        "    project=\"vi-alpaca-preference\",\n",
        "    name=\"llama-3.2-1b-4bit-dpo\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "30ea761c",
      "metadata": {
        "id": "30ea761c",
        "outputId": "82f69757-753e-4b37-ea93-fc5e7e028f2e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2032' max='2032' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2032/2032 10:27:27, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.496700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.303000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>0.295100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>0.283300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.272800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>0.270000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1400</td>\n",
              "      <td>0.250200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1600</td>\n",
              "      <td>0.253900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1800</td>\n",
              "      <td>0.258600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>0.246600</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=2032, training_loss=0.2921085744861543, metrics={'train_runtime': 37665.8776, 'train_samples_per_second': 3.452, 'train_steps_per_second': 0.054, 'total_flos': 0.0, 'train_loss': 0.2921085744861543, 'epoch': 2.0})"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "DPO_OUTPUT_DIR = f\"Llama-3.2-1B-Instruct-dpo\"\n",
        "dpo_args = DPOConfig(\n",
        "    **{ **hyperparameters, \"output_dir\":\n",
        "       DPO_OUTPUT_DIR, \"max_length\": MAX_LENGTH }\n",
        ")\n",
        "\n",
        "dpo_trainer = DPOTrainer(\n",
        "    base_model,\n",
        "    args=dpo_args,\n",
        "    train_dataset=dpo_dataset['train'],\n",
        "    processing_class=tokenizer,\n",
        "    peft_config=peft_config,\n",
        ")\n",
        "dpo_trainer.train()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "CoMA",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}