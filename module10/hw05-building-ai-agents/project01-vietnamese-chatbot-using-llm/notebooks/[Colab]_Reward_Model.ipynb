{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "449e8171",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModel,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoModelForSequenceClassification\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "445f5076",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "samples = [\n",
    "    {\n",
    "        \"prompt\": \"What is the capital of France?\",\n",
    "        \"chosen\": \"Paris is the capital of France.\",\n",
    "        \"rejected\": \"France is a city in Paris.\"\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"What is 2 + 2?\",\n",
    "        \"chosen\": \"The answer is 4.\",\n",
    "        \"rejected\": \"2 plus 2 equals 22.\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "14772c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RewardModel(nn.Module):\n",
    "    def __init__(self, model_name=\"meta-llama/Llama-3.2-1B\"):\n",
    "        super().__init__()\n",
    "        self.transformer = AutoModel.from_pretrained(model_name)\n",
    "        self.value_head = nn.Linear(self.transformer.config.hidden_size, 1)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        outputs = self.transformer(\n",
    "            input_ids=input_ids, attention_mask=attention_mask\n",
    "        )\n",
    "        last_hidden = outputs.last_hidden_state\n",
    "        value = self.value_head(last_hidden[:, -1, :]) # B x S x E\n",
    "        return value.squeeze(-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b9e2b40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RewardModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "edfeec10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_batch(samples):\n",
    "    chosen_texts = [s[\"prompt\"] + \" \" + s[\"chosen\"] for s in samples]\n",
    "    rejected_texts = [s[\"prompt\"] + \" \" + s[\"rejected\"] for s in samples]\n",
    "\n",
    "    chosen = tokenizer(\n",
    "        chosen_texts, padding=True, truncation=True, return_tensors=\"pt\"\n",
    "    )\n",
    "    rejected = tokenizer(\n",
    "        rejected_texts, padding=True, truncation=True, return_tensors=\"pt\"\n",
    "    )\n",
    "    return chosen, rejected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "da5c885f",
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen, rejected = encode_batch(samples)\n",
    "chosen = {k: v for k, v in chosen.items()}\n",
    "rejected = {k: v for k, v in rejected.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9c9f054b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 | Loss: 2.2268\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=2e-5)\n",
    "loss_fn = nn.MarginRankingLoss(margin=1.0)\n",
    "\n",
    "for epoch in range(1):\n",
    "    model.train()\n",
    "    r_chosen = model(**chosen)\n",
    "    r_rejected = model(**rejected)\n",
    "    target = torch.ones_like(r_chosen)\n",
    "\n",
    "    loss = loss_fn(r_chosen, r_rejected, target)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch} | Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "11f5fefe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "675f2edf711c492798a04e9e528f88ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12a8785a4d6e477dba78d0746025c081",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cdf365663444e3ba71ac408c8641ee4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08fbfce7b1c042b68289958116d299a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5311009fbde4f0aa2876689a7929f00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd9d67adef1e4527b3e1ad0e2ccc5d7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d90436c30a6b48e98fd7128433f33329",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPT2ForSequenceClassification(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (score): Linear(in_features=768, out_features=1, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Policy model\n",
    "policy = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "policy_ref = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "policy.train()\n",
    "policy_ref.eval()\n",
    "\n",
    "# Reward model\n",
    "reward_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name, num_labels=1\n",
    ")\n",
    "reward_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84deb6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"I am studying\"\n",
    "inputs = tokenizer(\n",
    "    prompt, return_tensors=\"pt\", padding=True\n",
    ")\n",
    "input_ids = inputs[\"input_ids\"]\n",
    "\n",
    "with torch.no_grad():\n",
    "    gen_ids = policy.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_new_tokens=20,\n",
    "        do_sample=True,\n",
    "        top_k=50,\n",
    "        temperature=1.0,\n",
    "        pad_token_id=tokenizer.pad_token_id\n",
    "    )\n",
    "response_ids = gen_ids[:, input_ids.shape[-1]:]\n",
    "query_response = torch.cat([input_ids, response_ids], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6cab6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# log-probs\n",
    "def get_log_prob_sum(model, input_ids):\n",
    "    labels = input_ids.clone()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, labels=labels)\n",
    "        loss = outputs.loss  # average negative log-likelihood\n",
    "    return -loss  # return log-likelihood\n",
    "\n",
    "logprob_policy = get_log_prob_sum(policy, query_response)\n",
    "logprob_ref = get_log_prob_sum(policy_ref, query_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429ede5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "logprob_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42db0baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "logprob_ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9378190",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reward score\n",
    "with torch.no_grad():\n",
    "    reward_inputs = tokenizer(\n",
    "        tokenizer.decode(query_response[0], skip_special_tokens=True),\n",
    "        return_tensors=\"pt\", truncation=True, padding=True\n",
    "    )\n",
    "    reward = reward_model(**reward_inputs).logits.squeeze().detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1f1fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba51dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "advantage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e84b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_clip_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bfdc4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# advantage, PPO loss, KL loss\n",
    "baseline = reward.detach()\n",
    "advantage = reward - baseline\n",
    "log_ratio = logprob_policy - logprob_ref\n",
    "ratio = torch.exp(log_ratio)\n",
    "\n",
    "# PPO-clip loss\n",
    "clip_eps = 0.2\n",
    "loss1 = ratio * advantage\n",
    "loss2 = torch.clamp(ratio, 1 - clip_eps, 1 + clip_eps) * advantage\n",
    "ppo_clip_loss = -torch.min(loss1, loss2)\n",
    "\n",
    "# KL loss (optional penalty)\n",
    "kl_loss = torch.mean(log_ratio**2)\n",
    "\n",
    "# Loss\n",
    "kl_coef = 0.01  # KL-Pen\n",
    "ppo_loss = ppo_clip_loss + kl_coef * kl_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102dd401",
   "metadata": {},
   "outputs": [],
   "source": [
    "kl_coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ea95f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901e8385",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383321f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GPT-2\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Policy model (trainable)\n",
    "policy_model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "policy_model.train()\n",
    "\n",
    "# Reference model (fixed, no grad)\n",
    "reference_model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "reference_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f70e8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = {\n",
    "    \"prompt\": \"The weather today is\",\n",
    "    \"chosen\": \" sunny and warm.\",\n",
    "    \"rejected\": \" not a banana.\"\n",
    "}\n",
    "\n",
    "prompt = sample[\"prompt\"]\n",
    "\n",
    "# Tokenize\n",
    "chosen = tokenizer(\n",
    "    prompt + sample[\"chosen\"], return_tensors=\"pt\", padding=True\n",
    ")\n",
    "rejected = tokenizer(\n",
    "    prompt + sample[\"rejected\"], return_tensors=\"pt\", padding=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bce6a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward pass - policy model\n",
    "policy_chosen_outputs = policy_model(**chosen)\n",
    "policy_rejected_outputs = policy_model(**rejected)\n",
    "\n",
    "# Forward pass - reference model (no_grad)\n",
    "with torch.no_grad():\n",
    "    ref_chosen_outputs = reference_model(**chosen)\n",
    "    ref_rejected_outputs = reference_model(**rejected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817387b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_chosen_outputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb37cdb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get logits\n",
    "def compute_log_prob(outputs, inputs):\n",
    "    logits = outputs.logits[:, :-1, :]\n",
    "    labels = inputs[\"input_ids\"][:, 1:]\n",
    "    log_probs = torch.gather(\n",
    "        logits.log_softmax(dim=-1), dim=2, index=labels.unsqueeze(-1)\n",
    "    ).squeeze(-1).sum(dim=1)\n",
    "    return log_probs\n",
    "\n",
    "policy_chosen_logp = compute_log_prob(policy_chosen_outputs, chosen)\n",
    "policy_rejected_logp = compute_log_prob(policy_rejected_outputs, rejected)\n",
    "ref_chosen_logp = compute_log_prob(ref_chosen_outputs, chosen)\n",
    "ref_rejected_logp = compute_log_prob(ref_rejected_outputs, rejected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702f7a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_chosen_logp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e117bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_rejected_logp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f7cb98",
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_chosen_logp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04f1332",
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_rejected_logp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c44def",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DPO loss function\n",
    "beta = 0.1\n",
    "\n",
    "diff = (policy_chosen_logp - ref_chosen_logp) - (policy_rejected_logp - ref_rejected_logp)\n",
    "dpo_loss = -torch.nn.functional.logsigmoid(beta * diff).mean()\n",
    "dpo_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7bd4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "diff"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dpo-ppo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
