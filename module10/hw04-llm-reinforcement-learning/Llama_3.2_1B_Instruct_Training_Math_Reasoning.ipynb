{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e3135c0",
   "metadata": {
    "id": "9e3135c0"
   },
   "source": [
    "## **1. Install & import libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09a8b98",
   "metadata": {
    "id": "e09a8b98"
   },
   "outputs": [],
   "source": [
    "%pip install unsloth vllm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d65bf0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a8d65bf0",
    "outputId": "be1b7aa7-07ad-479d-b2ec-2c7a7b676d8f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-27 20:13:15 [__init__.py:239] Automatically detected platform cuda.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from vllm import SamplingParams\n",
    "from unsloth import FastLanguageModel\n",
    "from datasets import load_dataset, Dataset\n",
    "from trl import GRPOConfig, GRPOTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3905a3c6",
   "metadata": {
    "id": "3905a3c6"
   },
   "source": [
    "## **2. Load and Prepare LoRA-Enabled Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1310f61b",
   "metadata": {
    "id": "1310f61b"
   },
   "outputs": [],
   "source": [
    "max_seq_length = 2048\n",
    "lora_rank = 64\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"meta-llama/Llama-3.2-3B-Instruct\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    load_in_4bit=False,\n",
    "    fast_inference=True,\n",
    "    max_lora_rank=lora_rank,\n",
    "    gpu_memory_utilization=0.8,\n",
    ")\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=lora_rank,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_alpha=lora_rank,\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    "    random_state=3407,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196ff74a",
   "metadata": {
    "id": "196ff74a"
   },
   "source": [
    "## **3. Load & format dataset for reasoning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "NTtZu9eJ4UQ1",
   "metadata": {
    "id": "NTtZu9eJ4UQ1"
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset(\n",
    "    \"5CD-AI/Vietnamese-meta-math-MetaMathQA-40K-gg-translated\", split=\"train\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "GoSrwR9J5Ujd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GoSrwR9J5Ujd",
    "outputId": "069727ef-25bc-4abc-a391-87a84806ca9b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset structure: Dataset({\n",
      "    features: ['response_vi', 'query_vi', 'response_en', 'type', 'query_en'],\n",
      "    num_rows: 40000\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(\"Dataset structure:\", dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219uaM4O851L",
   "metadata": {
    "id": "219uaM4O851L"
   },
   "outputs": [],
   "source": [
    "answer_pattern = re.compile(\n",
    "    r\"(đáp án là:|đáp án là :|câu trả lời là:|câu trả lời là :)\\s*(.*)\", re.IGNORECASE\n",
    ")\n",
    "\n",
    "formatted_dataset = []\n",
    "for item in dataset:\n",
    "    response = item[\"response_vi\"].strip().lower()\n",
    "    match = answer_pattern.search(response)\n",
    "    if match:\n",
    "        answer = match.group(2).strip()\n",
    "        formatted_dataset.append({\"question\": item[\"query_vi\"], \"answer\": answer})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c9d639",
   "metadata": {
    "id": "52c9d639"
   },
   "outputs": [],
   "source": [
    "reasoning_start = \"<thinking>\"\n",
    "reasoning_end = \"</thinking>\"\n",
    "solution_start = \"<answer>\"\n",
    "solution_end = \"</answer>\"\n",
    "\n",
    "system_prompt = f\"\"\"You are given a problem.\n",
    "Think about the problem and provide your thought process.\n",
    "Place it between {reasoning_start} and {reasoning_end}.\n",
    "Then, provide your final answer between {solution_start}{solution_end}\"\"\"\n",
    "\n",
    "train_dataset = Dataset.from_list(formatted_dataset[:8000])\n",
    "train_dataset = train_dataset.map(\n",
    "    lambda x: {\n",
    "        \"prompt\": [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": x[\"question\"]},\n",
    "        ],\n",
    "        \"answer\": x[\"answer\"],\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03b16a9",
   "metadata": {
    "id": "e03b16a9"
   },
   "outputs": [],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26bbf618",
   "metadata": {
    "id": "26bbf618"
   },
   "source": [
    "## **4. Define reward functions**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "noX623SHPQP_",
   "metadata": {
    "id": "noX623SHPQP_"
   },
   "source": [
    "### **4.1 Match format**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qbLoqZCvPUp6",
   "metadata": {
    "id": "qbLoqZCvPUp6"
   },
   "outputs": [],
   "source": [
    "match_format = re.compile(\n",
    "    rf\"^[\\s]{{0,}}\"\n",
    "    rf\"{reasoning_start}.+?{reasoning_end}.*?\"\n",
    "    rf\"{solution_start}(.+?){solution_end}\"\n",
    "    rf\"[\\s]{{0,}}$\",\n",
    "    flags=re.MULTILINE | re.DOTALL,\n",
    ")\n",
    "\n",
    "\n",
    "# math exactly -> 3.0\n",
    "def match_format_exactly(completions, **kwargs):\n",
    "    scores = []\n",
    "    for completion in completions:\n",
    "        score = 0\n",
    "        response = completion[0][\"content\"]\n",
    "        if match_format.search(response) is not None:\n",
    "            score += 3.0\n",
    "        scores.append(score)\n",
    "    return scores\n",
    "\n",
    "\n",
    "def match_format_approximately(completions, **kwargs):\n",
    "    scores = []\n",
    "    for completion in completions:\n",
    "        score = 0\n",
    "        response = completion[0][\"content\"]\n",
    "        score += 0.5 if response.count(reasoning_start) == 1 else -1.0\n",
    "        score += 0.5 if response.count(reasoning_end) == 1 else -1.0\n",
    "        score += 0.5 if response.count(solution_start) == 1 else -1.0\n",
    "        score += 0.5 if response.count(solution_end) == 1 else -1.0\n",
    "        scores.append(score)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "AXdjSp--PVSf",
   "metadata": {
    "id": "AXdjSp--PVSf"
   },
   "source": [
    "### **4.2 Match Answer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea48239",
   "metadata": {
    "id": "eea48239"
   },
   "outputs": [],
   "source": [
    "match_numbers = re.compile(\n",
    "    solution_start + r\".*?(-?[\\d\\.\\,]{1,})\", flags=re.MULTILINE | re.DOTALL\n",
    ")\n",
    "\n",
    "\n",
    "def check_answer(prompts, completions, answer, **kwargs):\n",
    "    responses = [completion[0][\"content\"] for completion in completions]\n",
    "\n",
    "    extracted_responses = [\n",
    "        guess.group(1) if (guess := match_format.search(r)) is not None else None\n",
    "        for r in responses\n",
    "    ]\n",
    "\n",
    "    scores = []\n",
    "    for guess, true_answer in zip(extracted_responses, answer):\n",
    "        score = 0\n",
    "        if guess is None:\n",
    "            scores.append(0)\n",
    "            continue\n",
    "\n",
    "        if guess == true_answer:\n",
    "            score += 3.0\n",
    "        elif guess.strip() == true_answer.strip():\n",
    "            score += 1.5\n",
    "        else:\n",
    "            score -= 1.5\n",
    "        scores.append(score)\n",
    "    return scores\n",
    "\n",
    "\n",
    "def check_numbers(prompts, completions, answer, **kwargs):\n",
    "    question = prompts[0][-1][\"content\"]\n",
    "    responses = [completion[0][\"content\"] for completion in completions]\n",
    "\n",
    "    extracted_responses = [\n",
    "        guess.group(1) if (guess := match_numbers.search(r)) is not None else None\n",
    "        for r in responses\n",
    "    ]\n",
    "\n",
    "    # Print every 5 steps\n",
    "    count = getattr(check_numbers, \"counter\", 0) + 1\n",
    "    check_numbers.counter = count\n",
    "    if count % 5 == 0:\n",
    "        print(\n",
    "            \"*\" * 20,\n",
    "            f\"Question:{question}\",\n",
    "            f\"\\nResponse:\\n{responses[0]}\",\n",
    "            f\"\\nExtracted: {extracted_responses[0]}\",\n",
    "            f\"\\nGT Answer: {answer[0]}\",\n",
    "        )\n",
    "\n",
    "    scores = []\n",
    "    for guess, true_answer in zip(extracted_responses, answer):\n",
    "        if guess is None:\n",
    "            scores.append(0)\n",
    "            continue\n",
    "        # Convert to numbers\n",
    "        try:\n",
    "            true_answer = float(true_answer.strip())\n",
    "            guess = float(guess.strip().replace(\",\", \"\"))\n",
    "            scores.append(1.5 if guess == true_answer else -0.5)\n",
    "        except:\n",
    "            scores.append(0)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c320a4e",
   "metadata": {
    "id": "9c320a4e"
   },
   "source": [
    "## **5. Training (GRPO)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4900fe56",
   "metadata": {
    "id": "4900fe56"
   },
   "outputs": [],
   "source": [
    "max_len = max(\n",
    "    dataset.map(\n",
    "        lambda x: {\n",
    "            \"tokens\": tokenizer.apply_chat_template(\n",
    "                x[\"prompt\"], add_generation_prompt=True, tokenize=True\n",
    "            )\n",
    "        },\n",
    "        batched=True,\n",
    "    ).map(lambda x: {\"length\": len(x[\"tokens\"])})[\"length\"]\n",
    ")\n",
    "\n",
    "max_prompt_length = max_len + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a143e0",
   "metadata": {
    "id": "d7a143e0"
   },
   "outputs": [],
   "source": [
    "training_args = GRPOConfig(\n",
    "    learning_rate=5e-6,\n",
    "    weight_decay=5e-4,\n",
    "    warmup_ratio=0.1,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    optim=\"adamw_torch_fused\",\n",
    "    logging_steps=1,\n",
    "    per_device_train_batch_size=8,\n",
    "    gradient_accumulation_steps=64,\n",
    "    num_generations=8,\n",
    "    max_prompt_length=max_prompt_length,\n",
    "    max_completion_length=max_seq_length - max_prompt_length,\n",
    "    num_train_epochs=1,\n",
    "    max_steps=-1,\n",
    "    save_steps=20,\n",
    "    max_grad_norm=0.1,\n",
    "    report_to=\"wandb\",\n",
    "    output_dir=\"grpo_lora\",\n",
    ")\n",
    "\n",
    "trainer = GRPOTrainer(\n",
    "    model=model,\n",
    "    processing_class=tokenizer,\n",
    "    reward_funcs=[\n",
    "        match_format_exactly,\n",
    "        match_format_approximately,\n",
    "        check_answer,\n",
    "        check_numbers,\n",
    "    ],\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c0a0ef",
   "metadata": {
    "id": "73c0a0ef"
   },
   "source": [
    "## **6. Save model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92904bc0",
   "metadata": {
    "id": "92904bc0"
   },
   "outputs": [],
   "source": [
    "model.save_lora(\"saved_grpo_lora\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7af783a",
   "metadata": {
    "id": "c7af783a"
   },
   "source": [
    "## **7. Inference**\n",
    "\n",
    "### Original Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6c2755",
   "metadata": {
    "id": "7f6c2755"
   },
   "outputs": [],
   "source": [
    "idx = 0\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_prompt},\n",
    "    {\"role\": \"user\", \"content\": train_dataset[idx][\"question\"]},\n",
    "]\n",
    "sampling_params = SamplingParams(\n",
    "    temperature=0.8,\n",
    "    top_p=0.95,\n",
    "    max_tokens=1024,\n",
    ")\n",
    "\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    "    tokenize=False,\n",
    ")\n",
    "\n",
    "output = (\n",
    "    model.fast_generate(\n",
    "        [text],\n",
    "        sampling_params=sampling_params,\n",
    "        lora_request=None,\n",
    "    )[0]\n",
    "    .outputs[0]\n",
    "    .text\n",
    ")\n",
    "\n",
    "print(f\"Problem:\\n{train_dataset[idx]['question']}\")\n",
    "print(f\"Response:\\n{output}\")\n",
    "print(\"GT Answer:\", train_dataset[idx][\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6794fc0f",
   "metadata": {
    "id": "6794fc0f"
   },
   "source": [
    "### Load Lora and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7aaca54",
   "metadata": {
    "id": "c7aaca54"
   },
   "outputs": [],
   "source": [
    "path_lora = \"saved_grpo_lora\"\n",
    "output = (\n",
    "    model.fast_generate(\n",
    "        [text],\n",
    "        sampling_params=sampling_params,\n",
    "        lora_request=model.load_lora(path_lora),\n",
    "    )[0]\n",
    "    .outputs[0]\n",
    "    .text\n",
    ")\n",
    "\n",
    "print(f\"Problem:\\n{train_dataset[idx]['question']}\")\n",
    "print(f\"Response:\\n{output}\")\n",
    "print(\"GT Answer:\", train_dataset[idx][\"answer\"])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "reasoning_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}